{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PiEstimator: Sample Spark-Scala App in Jupyter\n",
    "\n",
    "Requires the Jupyter-Scala language Kernel, available from: (https://github.com/alexarchambault/jupyter-scala)[https://github.com/alexarchambault/jupyter-scala]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 new artifact(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158 new artifacts in macro\n",
      "158 new artifacts in runtime\n",
      "158 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add( \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\",\n",
    "             \"org.apache.spark\" %% \"spark-mllib\" % \"1.6.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates Pi in Spark .. Adapted from: http://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mPiEstimator\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "  * Created by malarconba001 on 6/11/2016.\n",
    "  *\n",
    "  * PiEstimator: Estimates Pi in Spark .. Adapted from: http://spark.apache.org/examples.html\n",
    "  *\n",
    "  * Usage:\n",
    "  *     In your scala console type:\n",
    "  *\n",
    "  *         import week1.PiEstimator\n",
    "  *         PiEstimator.run (1000)\n",
    "  *\n",
    "  */\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object PiEstimator {\n",
    "  def run(numSamples:Int):Double ={\n",
    "    val conf = new SparkConf()\n",
    "      .setAppName(\"week1-EstimatePi\")\n",
    "      .setMaster(\"local[3]\") // Launch 3 local cores\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    val count = sc.parallelize(1 to numSamples).map{i =>\n",
    "      val x = Math.random()\n",
    "      val y = Math.random()\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    sc.stop()\n",
    "    4.0 * count / numSamples\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/06/11 16:11:22 INFO SparkContext: Running Spark version 1.6.1\n",
      "16/06/11 16:11:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/11 16:11:24 INFO SecurityManager: Changing view acls to: malarconba001\n",
      "16/06/11 16:11:24 INFO SecurityManager: Changing modify acls to: malarconba001\n",
      "16/06/11 16:11:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(malarconba001); users with modify permissions: Set(malarconba001)\n",
      "16/06/11 16:11:26 INFO Utils: Successfully started service 'sparkDriver' on port 5882.\n",
      "16/06/11 16:11:26 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/06/11 16:11:26 INFO Remoting: Starting remoting\n",
      "16/06/11 16:11:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.16:5895]\n",
      "16/06/11 16:11:27 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 5895.\n",
      "16/06/11 16:11:27 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/06/11 16:11:27 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/06/11 16:11:27 INFO DiskBlockManager: Created local directory at C:\\Users\\malarconba001\\AppData\\Local\\Temp\\blockmgr-052da734-4407-4fb2-b063-6fd5654e74de\n",
      "16/06/11 16:11:27 INFO MemoryStore: MemoryStore started with capacity 1773.8 MB\n",
      "16/06/11 16:11:27 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/06/11 16:11:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/06/11 16:11:28 INFO SparkUI: Started SparkUI at http://192.168.1.16:4040\n",
      "16/06/11 16:11:28 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/06/11 16:11:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5911.\n",
      "16/06/11 16:11:28 INFO NettyBlockTransferService: Server created on 5911\n",
      "16/06/11 16:11:28 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/06/11 16:11:28 INFO BlockManagerMasterEndpoint: Registering block manager localhost:5911 with 1773.8 MB RAM, BlockManagerId(driver, localhost, 5911)\n",
      "16/06/11 16:11:28 INFO BlockManagerMaster: Registered BlockManager\n",
      "16/06/11 16:11:29 INFO SparkContext: Starting job: reduce at Main.scala:34\n",
      "16/06/11 16:11:29 INFO DAGScheduler: Got job 0 (reduce at Main.scala:34) with 3 output partitions\n",
      "16/06/11 16:11:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at Main.scala:34)\n",
      "16/06/11 16:11:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/06/11 16:11:29 INFO DAGScheduler: Missing parents: List()\n",
      "16/06/11 16:11:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Main.scala:30), which has no missing parents\n",
      "16/06/11 16:11:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1880.0 B, free 1880.0 B)\n",
      "16/06/11 16:11:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1223.0 B, free 3.0 KB)\n",
      "16/06/11 16:11:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:5911 (size: 1223.0 B, free: 1773.7 MB)\n",
      "16/06/11 16:11:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n",
      "16/06/11 16:11:30 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Main.scala:30)\n",
      "16/06/11 16:11:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2078 bytes)\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2078 bytes)\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2,PROCESS_LOCAL, 2135 bytes)\n",
      "16/06/11 16:11:30 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "16/06/11 16:11:30 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "16/06/11 16:11:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "16/06/11 16:11:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1031 bytes result sent to driver\n",
      "16/06/11 16:11:30 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1031 bytes result sent to driver\n",
      "16/06/11 16:11:30 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1031 bytes result sent to driver\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 139 ms on localhost (1/3)\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 155 ms on localhost (2/3)\n",
      "16/06/11 16:11:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 197 ms on localhost (3/3)\n",
      "16/06/11 16:11:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "16/06/11 16:11:30 INFO DAGScheduler: ResultStage 0 (reduce at Main.scala:34) finished in 0.243 s\n",
      "16/06/11 16:11:30 INFO DAGScheduler: Job 0 finished: reduce at Main.scala:34, took 0.809810 s\n",
      "16/06/11 16:11:30 INFO SparkUI: Stopped Spark web UI at http://192.168.1.16:4040\n",
      "16/06/11 16:11:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "16/06/11 16:11:30 INFO MemoryStore: MemoryStore cleared\n",
      "16/06/11 16:11:30 INFO BlockManager: BlockManager stopped\n",
      "16/06/11 16:11:30 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "16/06/11 16:11:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "16/06/11 16:11:30 INFO SparkContext: Successfully stopped SparkContext\n",
      "16/06/11 16:11:30 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpi\u001b[0m: Double = \u001b[32m3.13728\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val pi = PiEstimator.run(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.13728\r\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Pi is roughly \" +pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
