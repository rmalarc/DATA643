{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "The goal of this assignment is to help you build your intuition about recommender systems, with a basic soup to nuts implementation coded “from scratch.”\n",
    "\n",
    "Your task is to build a very basic recommender system, first by writing your own functions, then by replacing those functions with those provided in an R Package or a Python library (such as scikit-learn).\n",
    "\n",
    "* You should very briefly first describe the recommender system that you’re going to build out from a business perspective, e.g. “This system recommends movies to users.”\n",
    "* You can find a dataset, or build out your own toy dataset and load into (for example) an R or pandas dataframe, a Python dictionary or list of lists, (or other data structure of your choosing).\n",
    "* You can use either collaborative filtering, or a hybrid of content management and collaborative filtering. \n",
    "* You are encouraged to hand code at least your similarity function.\n",
    "* After you have built out your own code base, create an alternate version using packages or libraries.  Compare the results and performance.\n",
    "* You are also encouraged to think about how to best handle missing data.\n",
    "* Your code should be turned in an RMarkdown file or a Jupyter notebook, and posted to Github.\n",
    "\n",
    "\n",
    "** Requires the Jupyter-Scala language Kernel, available from: (https://github.com/alexarchambault/jupyter-scala)[https://github.com/alexarchambault/jupyter-scala]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add( \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\",\n",
    "             \"org.apache.spark\" %% \"spark-mllib\" % \"1.6.1\",\n",
    "              \"org.apache.spark\" %% \"spark-sql\" % \"1.6.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "## The Recommender System\n",
    "\n",
    "As I'm farily new to Spark and the whole data manipulation world in Scala, let's keep the problem simple. This is a system that recommends movies to users based on the dataset collected by the class survey.\n",
    "\n",
    "As part of this exercise, I will produce a manual similarity function and compare the performance against the collaborative filtering library in Spark\n",
    "\n",
    "## The Code\n",
    "\n",
    "### Firing up a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[0m: org.apache.spark.SparkConf = org.apache.spark.SparkConf@582f9904\n",
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2d1c95ef"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val conf = new SparkConf()\n",
    "  .setAppName(\"week1-EstimatePi\")\n",
    "  .setMaster(\"local\") \n",
    "val sc = new SparkContext(conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Transformations\n",
    "\n",
    "The objective here is to:\n",
    "\n",
    "* Load the `MovieRatings.csv` file\n",
    "* Transform into Zero filled matrix\n",
    "* Transform into Long-format data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcsv\u001b[0m: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at Main.scala:29\n",
       "\u001b[36mres53_1\u001b[0m: Array[Array[String]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mArray\u001b[0m(\n",
       "    \u001b[32m\"Critic\"\u001b[0m,\n",
       "    \u001b[32m\"CaptainAmerica\"\u001b[0m,\n",
       "    \u001b[32m\"Deadpool\"\u001b[0m,\n",
       "    \u001b[32m\"Frozen\"\u001b[0m,\n",
       "    \u001b[32m\"JungleBook\"\u001b[0m,\n",
       "    \u001b[32m\"PitchPerfect2\"\u001b[0m,\n",
       "    \u001b[32m\"StarWarsForce\"\u001b[0m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Burton\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Charley\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"3\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Dan\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Matt\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Max\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Nathan\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Param\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"1\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Parshu\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"3\"\u001b[0m),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Read the CSV file\n",
    "val csv = \n",
    "    sc\n",
    "        .textFile(\"MovieRatings.csv\")\n",
    "        .map(line => \n",
    "             line\n",
    "                 .replaceAll(\",$\",\", \")\n",
    "                 .split(\",\")\n",
    "                 .map(t => t.trim)\n",
    "            )\n",
    "csv.collect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming into Zero-filled Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmovies\u001b[0m: Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"CaptainAmerica\"\u001b[0m,\n",
       "  \u001b[32m\"Deadpool\"\u001b[0m,\n",
       "  \u001b[32m\"Frozen\"\u001b[0m,\n",
       "  \u001b[32m\"JungleBook\"\u001b[0m,\n",
       "  \u001b[32m\"PitchPerfect2\"\u001b[0m,\n",
       "  \u001b[32m\"StarWarsForce\"\u001b[0m\n",
       ")\n",
       "\u001b[36mcritics\u001b[0m: Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"Burton\"\u001b[0m,\n",
       "  \u001b[32m\"Charley\"\u001b[0m,\n",
       "  \u001b[32m\"Dan\"\u001b[0m,\n",
       "  \u001b[32m\"Dieudonne\"\u001b[0m,\n",
       "  \u001b[32m\"Matt\"\u001b[0m,\n",
       "  \u001b[32m\"Mauricio\"\u001b[0m,\n",
       "  \u001b[32m\"Max\"\u001b[0m,\n",
       "  \u001b[32m\"Nathan\"\u001b[0m,\n",
       "  \u001b[32m\"Param\"\u001b[0m,\n",
       "  \u001b[32m\"Parshu\"\u001b[0m,\n",
       "  \u001b[32m\"Prashanth\"\u001b[0m,\n",
       "  \u001b[32m\"Shipra\"\u001b[0m,\n",
       "  \u001b[32m\"Sreejaya\"\u001b[0m,\n",
       "  \u001b[32m\"Steve\"\u001b[0m,\n",
       "  \u001b[32m\"Vuthy\"\u001b[0m,\n",
       "  \u001b[32m\"Xingjia\"\u001b[0m\n",
       ")\n",
       "\u001b[36mmoviesPar\u001b[0m: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[4] at zipWithIndex at Main.scala:38\n",
       "\u001b[36mcriticsPar\u001b[0m: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[6] at zipWithIndex at Main.scala:41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//val movies = sc.parallelize(csv.first)\n",
    "val movies = csv.first.splitAt(1)._2\n",
    "val critics = csv.collect.map(_(0)).splitAt(1)._2\n",
    "\n",
    "// let's also make a parallelized version of those\n",
    "val moviesPar = sc.parallelize(movies).zipWithIndex\n",
    "val criticsPar = sc.parallelize(critics).zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mzeroFilledMatrix\u001b[0m: Array[Array[Double]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m1.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m0.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m)\n",
       ")\n",
       "\u001b[36mzeroFilledMatrixPar\u001b[0m: org.apache.spark.rdd.RDD[Array[Double]] = ParallelCollectionRDD[7] at parallelize at Main.scala:47\n",
       "\u001b[36mcriticMoviesMatrix\u001b[0m: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@2cf2ec0c\n",
       "\u001b[36mdataTransposed\u001b[0m: org.apache.spark.rdd.RDD[Seq[Double]] = ParallelCollectionRDD[9] at parallelize at Main.scala:53\n",
       "\u001b[36mmoviesCriticMatrix\u001b[0m: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@3d286cd8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val zeroFilledMatrix = \n",
    "    csv\n",
    "        .collect\n",
    "        .filterNot(r => r(1) == movies(0)) // filter out first row\n",
    "        .map(r => r.filterNot(value => critics contains value)) // map function that returns the record minus the critic name\n",
    "        .map(r => r.map(value => if (value == \"\") 0.00 else value.toDouble))\n",
    "\n",
    "// and now parallelize it\n",
    "val zeroFilledMatrixPar = sc.parallelize(zeroFilledMatrix)\n",
    "\n",
    "// now, let's convert it into a linalg matrix so we can perform linear algebra operations on it\n",
    "val criticMoviesMatrix = new RowMatrix(zeroFilledMatrixPar.map(line => Vectors.dense(line)))\n",
    "\n",
    "\n",
    "// also, let's have a transposed version of it:\n",
    "val dataTransposed =  sc.parallelize(zeroFilledMatrix.toSeq.transpose)\n",
    "\n",
    "val moviesCriticMatrix = new RowMatrix(dataTransposed.map(line => Vectors.dense(line.toArray)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming into a Long-format data structure\n",
    "\n",
    "For practical purposes, we'll do an index-based long format, meaning that the string names will be substituted for an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlongFormat\u001b[0m: Array[(Int, Int, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mratingsLong\u001b[0m: org.apache.spark.rdd.RDD[(Int, Int, Double)] = ParallelCollectionRDD[11] at parallelize at Main.scala:42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val longFormat = \n",
    "    csv\n",
    "        .collect\n",
    "        .filterNot(r => r(1) == movies(0)) // filter out first row\n",
    "        .flatMap(r=> (1 to movies.length-1).map(i=> (r(0),movies(i-1),r(i)))) // pivot each column sothat we have: (user,movie,rating)\n",
    "        .filter(r=> r._3 !=\"\") // filter out those unrated movies\n",
    "        .map(r=> (critics.indexOf(r._1),movies.indexOf(r._2),r._3.toDouble)) // convert the remaining rating to a double\n",
    "                           \n",
    "val ratingsLong = sc.parallelize(longFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building - Manual Recommendations System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic-Critic Similarity Model\n",
    "\n",
    "Let's now build a User-User similarity model based on the cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36muserUserCosineDistance\u001b[0m: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@44033cf3\n",
       "\u001b[36muserUserSimilarities\u001b[0m: Array[Seq[Any]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(Dieudonne, Param, 0.9859249803487347),\n",
       "  \u001b[33mList\u001b[0m(Max, Sreejaya, 0.9847319278346619),\n",
       "  \u001b[33mList\u001b[0m(Charley, Max, 0.9811873171500672),\n",
       "  \u001b[33mList\u001b[0m(Charley, Sreejaya, 0.9792633226865932),\n",
       "  \u001b[33mList\u001b[0m(Charley, Prashanth, 0.9610484599102903),\n",
       "  \u001b[33mList\u001b[0m(Parshu, Prashanth, 0.9600666937386864),\n",
       "  \u001b[33mList\u001b[0m(Parshu, Sreejaya, 0.955672134494952),\n",
       "  \u001b[33mList\u001b[0m(Charley, Parshu, 0.9474847084398104),\n",
       "  \u001b[33mList\u001b[0m(Max, Prashanth, 0.9410294354946785),\n",
       "  \u001b[33mList\u001b[0m(Max, Parshu, 0.9296599791147713),\n",
       "  \u001b[33mList\u001b[0m(Prashanth, Sreejaya, 0.9293555142631518),\n",
       "  \u001b[33mList\u001b[0m(Charley, Vuthy, 0.927771250724491),\n",
       "  \u001b[33mList\u001b[0m(Matt, Steve, 0.9091372900969895),\n",
       "  \u001b[33mList\u001b[0m(Shipra, Xingjia, 0.8999999999999999),\n",
       "  \u001b[33mList\u001b[0m(Sreejaya, Vuthy, 0.8866206949335731),\n",
       "  \u001b[33mList\u001b[0m(Dieudonne, Steve, 0.870388279778489),\n",
       "  \u001b[33mList\u001b[0m(Max, Param, 0.866578244826242),\n",
       "  \u001b[33mList\u001b[0m(Parshu, Vuthy, 0.8661218807416571),\n",
       "  \u001b[33mList\u001b[0m(Max, Vuthy, 0.8574929257125442),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val userUserCosineDistance = moviesCriticMatrix.columnSimilarities()\n",
    "\n",
    "val userUserSimilarities = userUserCosineDistance\n",
    "  .entries\n",
    "  .map {\n",
    "    case MatrixEntry(i, j, u) => (i, j, u) }\n",
    "  .collect\n",
    "  .map(r => Seq(critics(r._1.toInt), critics(r._2.toInt), r._3.toDouble))\n",
    "  .sortBy(-_(2).asInstanceOf[Double])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Together a Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the following strategy:\n",
    "\n",
    "* For a given user, find simiar users. A similar user is another user with a cosine distance of at least 0.6\n",
    "* Make a movie rating recommendation based on the average of the similar user-group\n",
    "\n",
    "First, let's Generate Movie ratings for all critics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mrecommendMoviesForUser\u001b[0m\n",
       "\u001b[36mpredictedCriticRatings\u001b[0m: Array[Array[Double]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m4.5\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m3.3333333333333335\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\n",
       "    \u001b[32m4.333333333333333\u001b[0m,\n",
       "    \u001b[32m4.375\u001b[0m,\n",
       "    \u001b[32m3.5555555555555554\u001b[0m,\n",
       "    \u001b[32m3.857142857142857\u001b[0m,\n",
       "    \u001b[32m2.8333333333333335\u001b[0m,\n",
       "    \u001b[32m4.333333333333333\u001b[0m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.4\u001b[0m, \u001b[32m4.4\u001b[0m, \u001b[32m3.5\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m2.6666666666666665\u001b[0m, \u001b[32m4.333333333333333\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.25\u001b[0m, \u001b[32m4.428571428571429\u001b[0m, \u001b[32m3.7142857142857144\u001b[0m, \u001b[32m3.8\u001b[0m, \u001b[32m2.4\u001b[0m, \u001b[32m4.2\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\n",
       "    \u001b[32m4.333333333333333\u001b[0m,\n",
       "    \u001b[32m4.285714285714286\u001b[0m,\n",
       "    \u001b[32m3.857142857142857\u001b[0m,\n",
       "    \u001b[32m3.6666666666666665\u001b[0m,\n",
       "    \u001b[32m2.8\u001b[0m,\n",
       "    \u001b[32m4.111111111111111\u001b[0m\n",
       "  ),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recommendMoviesForUser(user:String,userUserSimilarities:Array[Seq[Any]],ratings:Array[Array[Double]])={\n",
    "    val similarUsers = userUserSimilarities\n",
    "        .filter(r=> r(0) == user || r(1)==user).map(r=>(if (r(1)==user) r(0) else r(1), r(2).asInstanceOf[Double]))\n",
    "        .filter(_._2>0.6)\n",
    "        .map(r=> (critics.indexOf(r._1),r._2))\n",
    "        .sortBy(-_._2)\n",
    "\n",
    "    val similarRatings = ratings\n",
    "        .zipWithIndex // add user indexes\n",
    "        .filter(r=>similarUsers.map(_._1) contains r._2) // filter out based on the index\n",
    "        .map(_._1) // revert back to the array\n",
    "    val meanRating = (0 to (movies.length-1)).map{\n",
    "        m => \n",
    "            val nonZeroRatings = similarRatings.map(c => c(m)).filter(_>0)\n",
    "            nonZeroRatings.sum/nonZeroRatings.length\n",
    "    }.toArray\n",
    "    meanRating\n",
    "}\n",
    "\n",
    "\n",
    "val predictedCriticRatings = critics.map(c=>recommendMoviesForUser(c,userUserSimilarities,zeroFilledMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the mean squared error for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mMSE\u001b[0m: collection.immutable.IndexedSeq[Double] = \u001b[33mVector\u001b[0m(\n",
       "  \u001b[32m8.449074074074074\u001b[0m,\n",
       "  \u001b[32m0.6510305125136474\u001b[0m,\n",
       "  \u001b[32m8.087592592592593\u001b[0m,\n",
       "  \u001b[32m5.897015306122449\u001b[0m,\n",
       "  \u001b[32m6.133667590492988\u001b[0m,\n",
       "  \u001b[32m6.671012849584277\u001b[0m,\n",
       "  \u001b[32m0.8786625514403292\u001b[0m,\n",
       "  \u001b[32m9.922453703703702\u001b[0m,\n",
       "  \u001b[32m4.949259259259259\u001b[0m,\n",
       "  \u001b[32m1.3704329386075418\u001b[0m,\n",
       "  \u001b[32m2.003435715335517\u001b[0m,\n",
       "  \u001b[32m7.851481481481482\u001b[0m,\n",
       "  \u001b[32m1.0986684565801628\u001b[0m,\n",
       "  \u001b[32m7.296342592592592\u001b[0m,\n",
       "  \u001b[32m2.957535903250189\u001b[0m,\n",
       "  \u001b[32m8.578703703703704\u001b[0m\n",
       ")\n",
       "\u001b[36mres59_1\u001b[0m: Double = \u001b[32m5.174773076958407\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val MSE = (0 to critics.length-1).map{ \n",
    "    c => \n",
    "        val row = (0 to movies.length-1).map{ \n",
    "            m =>\n",
    "                val err = predictedCriticRatings(c)(m) - zeroFilledMatrix(c)(m)\n",
    "                err * err\n",
    "        }\n",
    "        row.sum / row.length\n",
    "}\n",
    "MSE.sum/MSE.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Models\n",
    "\n",
    "* User: Who should Mauricio go out to the movies with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36muser\u001b[0m: String = \u001b[32m\"Mauricio\"\u001b[0m\n",
       "\u001b[36mres60_1\u001b[0m: Array[(Any, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(Parshu, \u001b[32m0.8140806303599618\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Vuthy, \u001b[32m0.7888934916555406\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Sreejaya, \u001b[32m0.7754763931697963\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val user= \"Mauricio\"\n",
    "userUserSimilarities\n",
    "    .filter(r=> r(0) == user || r(1)==user).map(r=>(if (r(1)==user) r(0) else r(1), r(2).asInstanceOf[Double]))\n",
    "    .filter(_._2>0.75)\n",
    "    .sortBy(-_._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What movies should Mauricio watch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres61\u001b[0m: Array[(String, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m4.5\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m4.285714285714286\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"StarWarsForce\"\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JungleBook\"\u001b[0m, \u001b[32m3.6666666666666665\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m2.5\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recommendMoviesForUser(\"Mauricio\",userUserSimilarities,zeroFilledMatrix).zipWithIndex.map(r => (movies(r._2),r._1)).sortBy(-_._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collaborative Filtering ALS Model\n",
    "\n",
    "Based on: http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.ALS\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.MatrixFactorizationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.Rating\u001b[0m\n",
       "\u001b[36mratingsALS\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[17] at map at Main.scala:35\n",
       "\u001b[36mrank\u001b[0m: Int = \u001b[32m10\u001b[0m\n",
       "\u001b[36mnumIterations\u001b[0m: Int = \u001b[32m10\u001b[0m\n",
       "\u001b[36mmodel\u001b[0m: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@6b9230e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "\n",
    "val ratingsALS = ratingsLong.map(r=>Rating(r._1, r._2, r._3))\n",
    "\n",
    "// Build the recommendation model using ALS\n",
    "val rank = 10\n",
    "val numIterations = 10\n",
    "val model = ALS.train(ratingsALS, rank, numIterations, 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the ALS Model\n",
    "\n",
    "Let's get:\n",
    "\n",
    "* Recommend 5 movies for Mau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalStateException: SparkContext has been shutdown (SparkContext has been shutdown)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)",
      "  org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1.apply(PairRDDFunctions.scala:939)",
      "  org.apache.spark.rdd.PairRDDFunctions$$anonfun$lookup$1.apply(PairRDDFunctions.scala:929)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:316)",
      "  org.apache.spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:928)",
      "  org.apache.spark.mllib.recommendation.MatrixFactorizationModel.recommendProducts(MatrixFactorizationModel.scala:168)",
      "  cmd65$$user$$anonfun$2.apply(Main.scala:32)",
      "  cmd65$$user$$anonfun$2.apply(Main.scala:31)"
     ]
    }
   ],
   "source": [
    "//What movies \n",
    "model.recommendProducts(5,critics.indexOf(\"Mauricio\")).map(r => (critics(r.user), movies(r.product), r.rating)) \n",
    "\n",
    "model.recommendUsersForProducts(3).collect.flatMap(m=>m._2.map(r=>(movies(m._1),critics(r.user),r.rating)))\n",
    "\n",
    "model.recommendProductsForUsers(3).collect.flatMap(m=>m._2.map(r=>(movies(r.product),critics(m._1),r.rating)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalStateException: SparkContext has been shutdown (SparkContext has been shutdown)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)",
      "  org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)",
      "  org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1114)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)",
      "  org.apache.spark.rdd.RDD.withScope(RDD.scala:316)",
      "  org.apache.spark.rdd.RDD.aggregate(RDD.scala:1107)",
      "  org.apache.spark.mllib.recommendation.MatrixFactorizationModel.countApproxDistinctUserProduct(MatrixFactorizationModel.scala:96)",
      "  org.apache.spark.mllib.recommendation.MatrixFactorizationModel.predict(MatrixFactorizationModel.scala:126)",
      "  cmd64$$user$$anonfun$3.apply(Main.scala:40)",
      "  cmd64$$user$$anonfun$3.apply(Main.scala:39)"
     ]
    }
   ],
   "source": [
    "// Evaluate the model on rating data\n",
    "val criticsMovies = ratingsALS.map { case Rating(critic, movie, rate) =>\n",
    "  (critic, movie)\n",
    "}\n",
    "val predictions =\n",
    "  model.predict(criticsMovies).map { case Rating(critic, movie, rate) =>\n",
    "    ((critic, movie), rate)\n",
    "  }\n",
    "val ratesAndPredictions = ratingsALS.map { case Rating(critic, movie, rate) =>\n",
    "  ((critic, movie), rate)\n",
    "}.join(predictions)\n",
    "val MSE = ratesAndPredictions.map { case ((critic, movie), (r1, r2)) =>\n",
    "  val err = (r1 - r2)\n",
    "  err * err\n",
    "}.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clearly the ALS model (MSE=9.456951543917609E-5) beats the manual model (MSE=5.174773076958407)\n",
    "* Alternative recommendation strategies can be explored such as nearest neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
