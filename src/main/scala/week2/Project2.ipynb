{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "The goal of this assignment is for you to try out different ways of implementing and configuring a recommender, and to evaluate your different approaches.\n",
    "\n",
    "For project 2, you’re asked to take some recommendation data (such as your toy movie dataset, Movielens, or another Dataset of your choosing), and implement at least two different recommendation algorithms on the data.  For example, content-based, user-user CF, and/or item-item CF.  You should evaluate different approaches, using different algorithms, normalization techniques, similarity methods, neighborhood sizes, etc.  You don’t need to be exhaustive—these are just some suggested possibilities.  You may use whatever third party libraries you want.  Please provide at least one graph, and a textual summary of your evaluation.\n",
    "\n",
    "You may work in a small group.  Please submit a link to your GitHub repository for your Jupyter notebook or RMarkdown file.  Due end of day on Sunday June 26th.\n",
    "\n",
    "**Requires the Jupyter-Scala language Kernel, available from: https://github.com/alexarchambault/jupyter-scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new artifact(s)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add( \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\",\n",
    "             \"org.apache.spark\" %% \"spark-mllib\" % \"1.6.1\",\n",
    "              \"org.apache.spark\" %% \"spark-sql\" % \"1.6.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "## The Recommender System\n",
    "\n",
    "As I'm farily new to Spark and the whole data manipulation world in Scala, let's keep the problem simple. This is a system that recommends movies to users based on the dataset collected by the class survey.\n",
    "\n",
    "As part of this exercise, I will produce a manual similarity function and compare the performance against the collaborative filtering library in Spark\n",
    "\n",
    "## The Code\n",
    "\n",
    "### Firing up a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[0m: org.apache.spark.SparkConf = org.apache.spark.SparkConf@15593650\n",
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4af0eef9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val conf = new SparkConf()\n",
    "  .setAppName(\"week1-EstimatePi\")\n",
    "  .setMaster(\"local\") \n",
    "val sc = new SparkContext(conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Transformations\n",
    "\n",
    "The objective here is to:\n",
    "\n",
    "* Load the `MovieRatings.csv` file\n",
    "* Transform into Zero filled matrix\n",
    "* Transform into Long-format data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcsv\u001b[0m: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at Main.scala:29\n",
       "\u001b[36mres27_1\u001b[0m: Array[Array[String]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mArray\u001b[0m(\n",
       "    \u001b[32m\"Critic\"\u001b[0m,\n",
       "    \u001b[32m\"CaptainAmerica\"\u001b[0m,\n",
       "    \u001b[32m\"Deadpool\"\u001b[0m,\n",
       "    \u001b[32m\"Frozen\"\u001b[0m,\n",
       "    \u001b[32m\"JungleBook\"\u001b[0m,\n",
       "    \u001b[32m\"PitchPerfect2\"\u001b[0m,\n",
       "    \u001b[32m\"StarWarsForce\"\u001b[0m\n",
       "  ),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Burton\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Charley\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"3\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Dan\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Matt\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Max\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Nathan\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"4\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Param\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"1\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"\"\u001b[0m, \u001b[32m\"5\"\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m\"Parshu\"\u001b[0m, \u001b[32m\"4\"\u001b[0m, \u001b[32m\"3\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"5\"\u001b[0m, \u001b[32m\"2\"\u001b[0m, \u001b[32m\"3\"\u001b[0m),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Read the CSV file\n",
    "val csv = \n",
    "    sc\n",
    "        .textFile(\"../MovieRatings.csv\")\n",
    "        .map(line => \n",
    "             line\n",
    "                 .replaceAll(\",$\",\", \")\n",
    "                 .split(\",\")\n",
    "                 .map(t => t.trim)\n",
    "            )\n",
    "csv.collect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming into Zero-filled Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmovies\u001b[0m: Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"Critic\"\u001b[0m,\n",
       "  \u001b[32m\"CaptainAmerica\"\u001b[0m,\n",
       "  \u001b[32m\"Deadpool\"\u001b[0m,\n",
       "  \u001b[32m\"Frozen\"\u001b[0m,\n",
       "  \u001b[32m\"JungleBook\"\u001b[0m,\n",
       "  \u001b[32m\"PitchPerfect2\"\u001b[0m,\n",
       "  \u001b[32m\"StarWarsForce\"\u001b[0m\n",
       ")\n",
       "\u001b[36mcritics\u001b[0m: Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"Critic\"\u001b[0m,\n",
       "  \u001b[32m\"Burton\"\u001b[0m,\n",
       "  \u001b[32m\"Charley\"\u001b[0m,\n",
       "  \u001b[32m\"Dan\"\u001b[0m,\n",
       "  \u001b[32m\"Dieudonne\"\u001b[0m,\n",
       "  \u001b[32m\"Matt\"\u001b[0m,\n",
       "  \u001b[32m\"Mauricio\"\u001b[0m,\n",
       "  \u001b[32m\"Max\"\u001b[0m,\n",
       "  \u001b[32m\"Nathan\"\u001b[0m,\n",
       "  \u001b[32m\"Param\"\u001b[0m,\n",
       "  \u001b[32m\"Parshu\"\u001b[0m,\n",
       "  \u001b[32m\"Prashanth\"\u001b[0m,\n",
       "  \u001b[32m\"Shipra\"\u001b[0m,\n",
       "  \u001b[32m\"Sreejaya\"\u001b[0m,\n",
       "  \u001b[32m\"Steve\"\u001b[0m,\n",
       "  \u001b[32m\"Vuthy\"\u001b[0m,\n",
       "  \u001b[32m\"Xingjia\"\u001b[0m\n",
       ")\n",
       "\u001b[36mmoviesPar\u001b[0m: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[4] at zipWithIndex at Main.scala:38\n",
       "\u001b[36mcriticsPar\u001b[0m: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[6] at zipWithIndex at Main.scala:41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//val movies = sc.parallelize(csv.first)\n",
    "val movies = csv.first\n",
    "val critics = csv.collect.map(_(0))\n",
    "\n",
    "// let's also make a parallelized version of those\n",
    "val moviesPar = sc.parallelize(movies).zipWithIndex\n",
    "val criticsPar = sc.parallelize(critics).zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mzeroFilledMatrix\u001b[0m: Array[Array[Double]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m1.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m2.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m4.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m3.0\u001b[0m, \u001b[32m0.0\u001b[0m),\n",
       "  \u001b[33mArray\u001b[0m(\u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m5.0\u001b[0m, \u001b[32m0.0\u001b[0m, \u001b[32m0.0\u001b[0m)\n",
       ")\n",
       "\u001b[36mzeroFilledMatrixPar\u001b[0m: org.apache.spark.rdd.RDD[Array[Double]] = ParallelCollectionRDD[7] at parallelize at Main.scala:47\n",
       "\u001b[36mcriticMoviesMatrix\u001b[0m: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@17cc6a32\n",
       "\u001b[36mdataTransposed\u001b[0m: org.apache.spark.rdd.RDD[Seq[Double]] = ParallelCollectionRDD[9] at parallelize at Main.scala:53\n",
       "\u001b[36mmoviesCriticMatrix\u001b[0m: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@653f98e5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val zeroFilledMatrix = \n",
    "    csv\n",
    "        .collect\n",
    "        .filterNot(r => r(0) == movies(0)) // filter out first row\n",
    "        .map(r => r.filterNot(value => critics contains value)) // map function that returns the record minus the critic name\n",
    "        .map(r => r.map(value => if (value == \"\") 0.00 else value.toDouble))\n",
    "\n",
    "// and now parallelize it\n",
    "val zeroFilledMatrixPar = sc.parallelize(zeroFilledMatrix)\n",
    "\n",
    "// now, let's convert it into a linalg matrix so we can perform linear algebra operations on it\n",
    "val criticMoviesMatrix = new RowMatrix(zeroFilledMatrixPar.map(line => Vectors.dense(line)))\n",
    "\n",
    "\n",
    "// also, let's have a transposed version of it:\n",
    "val dataTransposed =  sc.parallelize(zeroFilledMatrix.toSeq.transpose)\n",
    "\n",
    "val moviesCriticMatrix = new RowMatrix(dataTransposed.map(line => Vectors.dense(line.toArray)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming into a Long-format data structure\n",
    "\n",
    "For practical purposes, we'll do an index-based long format, meaning that the string names will be substituted for an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlongFormat\u001b[0m: Array[(Int, Int, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m0\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mratingsLong\u001b[0m: org.apache.spark.rdd.RDD[(Int, Int, Double)] = ParallelCollectionRDD[11] at parallelize at Main.scala:42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val longFormat = \n",
    "    csv\n",
    "        .collect\n",
    "        .filterNot(r => r(0) == movies(0)) // filter out first row\n",
    "        .flatMap(r=> (1 to movies.length-1).map(i=> (r(0),movies(i-1),r(i)))) // pivot each column sothat we have: (user,movie,rating)\n",
    "        .filter(r=> r._3 !=\"\") // filter out those unrated movies\n",
    "        .map(r=> (critics.indexOf(r._1),movies.indexOf(r._2),r._3.toDouble)) // convert the remaining rating to a double\n",
    "                           \n",
    "val ratingsLong = sc.parallelize(longFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building - Cosine Distance Based\n",
    "\n",
    "Let's now build the following models using their respective cosine distances:\n",
    "\n",
    "* Movie-Movie collaborative filtering\n",
    "* User-User collaborative filtering\n",
    "\n",
    "\n",
    "### Movie-Movie  Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmoviesMoviesCosineDistance\u001b[0m: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@11921c07\n",
       "\u001b[36mmoviesMoviesSimilarities\u001b[0m: Array[Seq[Any]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(Deadpool, Frozen, 0.9164305818234458),\n",
       "  \u001b[33mList\u001b[0m(Critic, CaptainAmerica, 0.8011927448021527),\n",
       "  \u001b[33mList\u001b[0m(Critic, PitchPerfect2, 0.7649201061631455),\n",
       "  \u001b[33mList\u001b[0m(Critic, JungleBook, 0.7437115739277018),\n",
       "  \u001b[33mList\u001b[0m(Critic, Deadpool, 0.7406840835263832),\n",
       "  \u001b[33mList\u001b[0m(CaptainAmerica, PitchPerfect2, 0.7299810272285707),\n",
       "  \u001b[33mList\u001b[0m(Deadpool, JungleBook, 0.7191555984642707),\n",
       "  \u001b[33mList\u001b[0m(CaptainAmerica, Deadpool, 0.6802170238514358),\n",
       "  \u001b[33mList\u001b[0m(Critic, Frozen, 0.5992177600023244),\n",
       "  \u001b[33mList\u001b[0m(Frozen, JungleBook, 0.5913486717104743),\n",
       "  \u001b[33mList\u001b[0m(CaptainAmerica, JungleBook, 0.579267135696289),\n",
       "  \u001b[33mList\u001b[0m(Deadpool, PitchPerfect2, 0.5773720984150789),\n",
       "  \u001b[33mList\u001b[0m(CaptainAmerica, Frozen, 0.5689794478548152),\n",
       "  \u001b[33mList\u001b[0m(Frozen, PitchPerfect2, 0.5296627487933019),\n",
       "  \u001b[33mList\u001b[0m(JungleBook, PitchPerfect2, 0.43479904753372806)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val moviesMoviesCosineDistance = criticMoviesMatrix.columnSimilarities()\n",
    "\n",
    "val moviesMoviesSimilarities = moviesMoviesCosineDistance\n",
    "  .entries\n",
    "  .map {\n",
    "    case MatrixEntry(i, j, u) => (i, j, u) }\n",
    "  .collect\n",
    "  .map(r => Seq(movies(r._1.toInt), movies(r._2.toInt), r._3.toDouble))\n",
    "  .sortBy(-_(2).asInstanceOf[Double])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-User Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36muserUserCosineDistance\u001b[0m: org.apache.spark.mllib.linalg.distributed.CoordinateMatrix = org.apache.spark.mllib.linalg.distributed.CoordinateMatrix@62e53689\n",
       "\u001b[36muserUserSimilarities\u001b[0m: Array[Seq[Any]] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mList\u001b[0m(Dan, Nathan, 0.9859249803487347),\n",
       "  \u001b[33mList\u001b[0m(Mauricio, Shipra, 0.9847319278346619),\n",
       "  \u001b[33mList\u001b[0m(Burton, Mauricio, 0.9811873171500672),\n",
       "  \u001b[33mList\u001b[0m(Burton, Shipra, 0.9792633226865932),\n",
       "  \u001b[33mList\u001b[0m(Burton, Parshu, 0.9610484599102903),\n",
       "  \u001b[33mList\u001b[0m(Param, Parshu, 0.9600666937386864),\n",
       "  \u001b[33mList\u001b[0m(Param, Shipra, 0.955672134494952),\n",
       "  \u001b[33mList\u001b[0m(Burton, Param, 0.9474847084398104),\n",
       "  \u001b[33mList\u001b[0m(Mauricio, Parshu, 0.9410294354946785),\n",
       "  \u001b[33mList\u001b[0m(Mauricio, Param, 0.9296599791147713),\n",
       "  \u001b[33mList\u001b[0m(Parshu, Shipra, 0.9293555142631518),\n",
       "  \u001b[33mList\u001b[0m(Burton, Steve, 0.927771250724491),\n",
       "  \u001b[33mList\u001b[0m(Dieudonne, Sreejaya, 0.9091372900969895),\n",
       "  \u001b[33mList\u001b[0m(Prashanth, Vuthy, 0.8999999999999999),\n",
       "  \u001b[33mList\u001b[0m(Shipra, Steve, 0.8866206949335731),\n",
       "  \u001b[33mList\u001b[0m(Dan, Sreejaya, 0.870388279778489),\n",
       "  \u001b[33mList\u001b[0m(Mauricio, Nathan, 0.866578244826242),\n",
       "  \u001b[33mList\u001b[0m(Param, Steve, 0.8661218807416571),\n",
       "  \u001b[33mList\u001b[0m(Mauricio, Steve, 0.8574929257125442),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val userUserCosineDistance = moviesCriticMatrix.columnSimilarities()\n",
    "\n",
    "val userUserSimilarities = userUserCosineDistance\n",
    "  .entries\n",
    "  .map {\n",
    "    case MatrixEntry(i, j, u) => (i, j, u) }\n",
    "  .collect\n",
    "  .map(r => Seq(critics(r._1.toInt), critics(r._2.toInt), r._3.toDouble))\n",
    "  .sortBy(-_(2).asInstanceOf[Double])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Models\n",
    "\n",
    "* User: Who should Mauricio go out to the movies with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36muser\u001b[0m: String = \u001b[32m\"Mauricio\"\u001b[0m\n",
       "\u001b[36mres33_1\u001b[0m: Array[(Any, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(Shipra, \u001b[32m0.9847319278346619\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Burton, \u001b[32m0.9811873171500672\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Parshu, \u001b[32m0.9410294354946785\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Param, \u001b[32m0.9296599791147713\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Nathan, \u001b[32m0.866578244826242\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Steve, \u001b[32m0.8574929257125442\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Dan, \u001b[32m0.8123623944599234\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(Dieudonne, \u001b[32m0.8081220356417687\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val user= \"Mauricio\"\n",
    "userUserSimilarities\n",
    "    .filter(r=> r(0) == user || r(1)==user).map(r=>(if (r(1)==user) r(0) else r(1), r(2).asInstanceOf[Double]))\n",
    "    .filter(_._2>0.75)\n",
    "    .sortBy(-_._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If I liked Frozen, what other movies should I watch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmovie\u001b[0m: String = \u001b[32m\"Frozen\"\u001b[0m\n",
       "\u001b[36mres34_1\u001b[0m: Array[(Any, Double)] = \u001b[33mArray\u001b[0m(\u001b[33m\u001b[0m(Deadpool, \u001b[32m0.9164305818234458\u001b[0m))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val movie= \"Frozen\"\n",
    "moviesMoviesSimilarities\n",
    "    .filter(r=> r(0) == movie || r(1)==movie).map(r=>(if (r(1)==movie) r(0) else r(1), r(2).asInstanceOf[Double]))\n",
    "    .filter(_._2>0.75)\n",
    "    .sortBy(-_._2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collaborative Filtering ALS Model\n",
    "\n",
    "Based on: http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 1.7300149322469586E-4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.ALS\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.MatrixFactorizationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.Rating\u001b[0m\n",
       "\u001b[36mratingsALS\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[22] at map at Main.scala:42\n",
       "\u001b[36mrank\u001b[0m: Int = \u001b[32m10\u001b[0m\n",
       "\u001b[36mnumIterations\u001b[0m: Int = \u001b[32m10\u001b[0m\n",
       "\u001b[36mmodel\u001b[0m: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@60b785dd\n",
       "\u001b[36mcriticsMovies\u001b[0m: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[230] at map at Main.scala:54\n",
       "\u001b[36mpredictions\u001b[0m: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[240] at map at Main.scala:59\n",
       "\u001b[36mratesAndPredictions\u001b[0m: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[244] at join at Main.scala:66\n",
       "\u001b[36mMSE\u001b[0m: Double = \u001b[32m1.7300149322469586E-4\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "\n",
    "val ratingsALS = ratingsLong.map(r=>Rating(r._1, r._2, r._3))\n",
    "\n",
    "// Build the recommendation model using ALS\n",
    "val rank = 10\n",
    "val numIterations = 10\n",
    "val model = ALS.train(ratingsALS, rank, numIterations, 0.01)\n",
    "\n",
    "// Evaluate the model on rating data\n",
    "val criticsMovies = ratingsALS.map { case Rating(critic, movie, rate) =>\n",
    "  (critic, movie)\n",
    "}\n",
    "val predictions =\n",
    "  model.predict(criticsMovies).map { case Rating(critic, movie, rate) =>\n",
    "    ((critic, movie), rate)\n",
    "  }\n",
    "val ratesAndPredictions = ratingsALS.map { case Rating(critic, movie, rate) =>\n",
    "  ((critic, movie), rate)\n",
    "}.join(predictions)\n",
    "val MSE = ratesAndPredictions.map { case ((critic, movie), (r1, r2)) =>\n",
    "  val err = (r1 - r2)\n",
    "  err * err\n",
    "}.mean()\n",
    "\n",
    "println(\"Mean Squared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36_0\u001b[0m: Array[(String, String, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m4.55839706531505\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"Critic\"\u001b[0m, \u001b[32m4.000934381708938\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"JungleBook\"\u001b[0m, \u001b[32m3.9826942547896538\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m3.896175989914277\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m3.0055283089099647\u001b[0m)\n",
       ")\n",
       "\u001b[36mres36_1\u001b[0m: Array[(String, String, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JungleBook\"\u001b[0m, \u001b[32m\"Mauricio\"\u001b[0m, \u001b[32m3.9826942547896538\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JungleBook\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m3.981912103579872\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"JungleBook\"\u001b[0m, \u001b[32m\"Vuthy\"\u001b[0m, \u001b[32m2.9946114143957767\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m5.015867105191404\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m4.985885047261088\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m4.9770757945934605\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m5.006493164406723\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m5.001166196717815\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Dan\"\u001b[0m, \u001b[32m4.999004400387928\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Parshu\"\u001b[0m, \u001b[32m5.002470638416293\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Xingjia\"\u001b[0m, \u001b[32m4.999274699833141\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m4.998333628212166\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m5.000081591642868\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Dan\"\u001b[0m, \u001b[32m4.998303347767423\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Param\"\u001b[0m, \u001b[32m4.994296385900464\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m5.001733212512384\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m\"Xingjia\"\u001b[0m, \u001b[32m4.997725333139232\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m\"Parshu\"\u001b[0m, \u001b[32m4.993803754693575\u001b[0m)\n",
       ")\n",
       "\u001b[36mres36_2\u001b[0m: Array[(String, String, Double)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m5.015867105191404\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m5.006493164406723\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Sreejaya\"\u001b[0m, \u001b[32m4.984650429085939\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Vuthy\"\u001b[0m, \u001b[32m4.98280450314104\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Vuthy\"\u001b[0m, \u001b[32m4.013976007310307\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Vuthy\"\u001b[0m, \u001b[32m3.701884462220896\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m5.000081591642868\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m4.985885047261088\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Dieudonne\"\u001b[0m, \u001b[32m4.716521216286608\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Xingjia\"\u001b[0m, \u001b[32m4.999274699833141\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m\"Xingjia\"\u001b[0m, \u001b[32m4.997725333139232\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Xingjia\"\u001b[0m, \u001b[32m4.661032770484523\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Deadpool\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m5.001733212512384\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m5.001166196717815\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Frozen\"\u001b[0m, \u001b[32m\"Prashanth\"\u001b[0m, \u001b[32m4.998333628212166\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"PitchPerfect2\"\u001b[0m, \u001b[32m\"Steve\"\u001b[0m, \u001b[32m3.999101470725706\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Steve\"\u001b[0m, \u001b[32m3.9987419381991303\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"CaptainAmerica\"\u001b[0m, \u001b[32m\"Steve\"\u001b[0m, \u001b[32m3.9434070507244128\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Critic\"\u001b[0m, \u001b[32m\"Burton\"\u001b[0m, \u001b[32m4.155743340143742\u001b[0m),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.recommendProducts(critics.indexOf(user),5).map(r => (critics(r.user), movies(r.product), r.rating)) \n",
    "model.recommendUsersForProducts(3).collect.flatMap(m=>m._2.map(r=>(movies(m._1),critics(r.user),r.rating)))\n",
    "model.recommendProductsForUsers(3).collect.flatMap(m=>m._2.map(r=>(movies(r.product),critics(m._1),r.rating)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmoviesCriticMatrix\u001b[0m: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@41baebb1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val moviesCriticMatrix = new RowMatrix(sc.parallelize(Array(Array(1.00,4.0),Array(1.0,4.0))).map(line => Vectors.dense(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres50\u001b[0m: Array[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = \u001b[33mArray\u001b[0m(\u001b[33mMatrixEntry\u001b[0m(\u001b[32m0L\u001b[0m, \u001b[32m1L\u001b[0m, \u001b[32m0.9999999999999998\u001b[0m))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moviesCriticMatrix.columnSimilarities().entries.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
