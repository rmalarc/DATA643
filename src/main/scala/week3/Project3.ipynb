{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "The goal of this assignment is give you practice working with Singular Value Decomposition.\n",
    "\n",
    "Your task is implement a matrix factorization method—such as singular value decomposition (SVD) or Alternating Least Squares (ALS)—in the context of a recommender system.\n",
    "\n",
    "You may approach this in a large number of ways.  You are welcome to start with an existing recommender system written by yourself or someone else (always citing your sources, so that you can be graded on what you added, not what you found).\n",
    "\n",
    "Here is one example.  Suppose you start with (or create) a collaborative filtering system against (a subset of) the MovieLens database or our toy dataset.  You could create a content-based system, where you populate your item profiles by pulling text information for specific movies from a source like imdb, applying text processing techniques (like TF-IDF), then using SVD and topic modeling to create a set of features derived from the text.\n",
    "\n",
    "An extra intermediate step could be to take text that was pre-classified, e.g. “fighting” or “singing” and build out two “explainable” features.  SVD builds features that may or may not map neatly to movie genres or news topics.\n",
    "\n",
    "**Requires the Jupyter-Scala language Kernel, available from: https://github.com/alexarchambault/jupyter-scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 new artifact(s)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161 new artifacts in macro\n",
      "161 new artifacts in runtime\n",
      "161 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add( \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\",\n",
    "             \"org.apache.spark\" %% \"spark-mllib\" % \"1.6.1\",\n",
    "              \"org.apache.spark\" %% \"spark-sql\" % \"1.6.1\",\n",
    "             \"co.theasi\" % \"plotly_2.10\" % \"0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "## The Recommender System\n",
    "\n",
    "This week I'll try loading a more complicated dataset: Plain text. For the purpose of this exercise, I'll use t\n",
    "\n",
    "## The Code\n",
    "\n",
    "### Firing up a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.types._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/06/30 18:52:40 INFO SparkContext: Running Spark version 1.6.1\n",
      "16/06/30 18:52:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/30 18:52:41 INFO SecurityManager: Changing view acls to: malarconba001\n",
      "16/06/30 18:52:41 INFO SecurityManager: Changing modify acls to: malarconba001\n",
      "16/06/30 18:52:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(malarconba001); users with modify permissions: Set(malarconba001)\n",
      "16/06/30 18:52:43 INFO Utils: Successfully started service 'sparkDriver' on port 15597.\n",
      "16/06/30 18:52:43 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/06/30 18:52:43 INFO Remoting: Starting remoting\n",
      "16/06/30 18:52:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.15:15610]\n",
      "16/06/30 18:52:44 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 15610.\n",
      "16/06/30 18:52:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/06/30 18:52:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/06/30 18:52:44 INFO DiskBlockManager: Created local directory at C:\\Users\\malarconba001\\AppData\\Local\\Temp\\blockmgr-97d48bdf-a10b-40a4-9d87-2b6a39314454\n",
      "16/06/30 18:52:44 INFO MemoryStore: MemoryStore started with capacity 1773.8 MB\n",
      "16/06/30 18:52:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/06/30 18:52:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "16/06/30 18:52:45 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "16/06/30 18:52:45 INFO SparkUI: Started SparkUI at http://192.168.1.15:4041\n",
      "16/06/30 18:52:45 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/06/30 18:52:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15617.\n",
      "16/06/30 18:52:45 INFO NettyBlockTransferService: Server created on 15617\n",
      "16/06/30 18:52:45 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/06/30 18:52:45 INFO BlockManagerMasterEndpoint: Registering block manager localhost:15617 with 1773.8 MB RAM, BlockManagerId(driver, localhost, 15617)\n",
      "16/06/30 18:52:45 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[0m: org.apache.spark.SparkConf = org.apache.spark.SparkConf@455e7809\n",
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@18e637dd"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    val conf = new SparkConf()\n",
    "      .setAppName(\"week1-EstimatePi\")\n",
    "      .setMaster(\"local\") \n",
    "\n",
    "    val sc = new SparkContext(conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Transformations\n",
    "\n",
    "The objective here is to:\n",
    "\n",
    "* Load the http://mc6help.tripod.com/RecipeLibrary/AllAppetizerRecipes.txt file\n",
    "* Transform into Zero filled matrix\n",
    "* Transform into Long-format data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcsv\u001b[0m: org.apache.spark.rdd.RDD[(String, Boolean, Long)] = MapPartitionsRDD[5] at map at Main.scala:30\n",
       "\u001b[36mres3_1\u001b[0m: Array[(String, Boolean, Long)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"* Exported from MasterCook *\"\u001b[0m, \u001b[32mtrue\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m1L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Barbecue Pecans\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m2L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m3L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Recipe By     : Possum Kingdom Lake Cookbook\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m4L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Serving Size  : 25    Preparation Time : 0:00\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m5L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Categories    :\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m6L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Amount  Measure       Ingredient -- Preparation Method\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m7L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"--------  ------------  --------------------------------\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m8L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"2   tablespoons  butter\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m9L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"1/4           cup  Worcestershire sauce\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m10L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"1    tablespoon  catsup\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m11L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"6        dashes  Hot sauce\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m12L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"4          cups  Pecans -- halves\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m13L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"salt -- to taste\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m14L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"\"\u001b[0m, \u001b[32mfalse\u001b[0m, \u001b[32m15L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"Melt butter in a large saucepan; add Worcestershire sauce, , catsup, and hot sauce.\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "val csv = \n",
    "    sc\n",
    "        .textFile(\"AllAppetizerRecipes.txt\")\n",
    "        .map(t => t.trim)\n",
    "        .map(t => (t,t==\"* Exported from MasterCook *\") ) // add boolean if we have a record delimiter\n",
    "        .zipWithIndex // add record id\n",
    "        .map(r=>(r._1._1,r._1._2,r._2)) // flatten the nested index\n",
    "csv.take(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, sample, we can see that the text file gets imported a line at a time per record. However, the recipes file is formatted as follows:\n",
    "\n",
    "```\n",
    "* Exported from MasterCook *\n",
    "\n",
    "                     Barbecue Pecans\n",
    "\n",
    "Recipe By     : Possum Kingdom Lake Cookbook\n",
    "Serving Size  : 25    Preparation Time : 0:00\n",
    "Categories    : \n",
    "  Amount  Measure       Ingredient -- Preparation Method\n",
    "--------  ------------  --------------------------------\n",
    "       2   tablespoons  butter\n",
    "     1/4           cup  Worcestershire sauce\n",
    "       1    tablespoon  catsup\n",
    "       6        dashes  Hot sauce\n",
    "       4          cups  Pecans -- halves\n",
    "                        salt -- to taste\n",
    "\n",
    "Melt butter in a large saucepan; add Worcestershire sauce, , catsup, and hot sauce.  \n",
    "\n",
    "Stir in nuts; spoon into a glass baking dish, spreading evenly.  toast at 400 degrees about 20 minutes, stirring frequently.  \n",
    "\n",
    "Turn out on absorbent towels, and sprinkle with salt.\n",
    "\n",
    "                                    - - - - - - - - - - - - - - - - - - - \n",
    "```\n",
    "\n",
    "The goal here is to map the file into: RecipeIngredients(RecipeName, RecipeText) \n",
    "\n",
    "Where:\n",
    "\n",
    "* Record Break: is the line: ```* Exported from MasterCook *```\n",
    "* An entire record is composed of the lines between record breaks\n",
    "* RecipeName: The third line in the record\n",
    "* RecipeText: The concatenated lines of the recipe record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecordIndexes\u001b[0m: org.apache.spark.rdd.RDD[(Long, Long)] = UnionRDD[12] at $plus$plus at Main.scala:27\n",
       "\u001b[36mrecordIndexesOffset\u001b[0m: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[13] at map at Main.scala:30\n",
       "\u001b[36mrecipeIndexTable\u001b[0m: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[17] at flatMap at Main.scala:33\n",
       "\u001b[36mres4_3\u001b[0m: Array[(Long, Long)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m0L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m4L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m5L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m6L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m7L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m8L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m9L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m10L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m11L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m12L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m13L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m14L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m15L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m16L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m17L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m18L\u001b[0m, \u001b[32m0L\u001b[0m),\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// The goal with this section is to end up with a recipeIndexTable collection that looks like: recipeIndexTable(RecordId,RecipeID)\n",
    "\n",
    "// Generate a recordIndex table (RecipeId, LineId) by relying in the presence and offset of the record break\n",
    "val recordIndexes =  csv.filter(_._2).map(_._3).zipWithIndex.map(r=>(r._2,r._1)) ++ sc.parallelize(Seq((csv.filter(_._2).count,csv.count)))\n",
    "\n",
    "// Now, let's just create an offset recordIndexes Table \n",
    "val recordIndexesOffset = recordIndexes.map(r=>(r._1-1,r._2-1))\n",
    "\n",
    "// and join it with the record indexes so we have the recipeIndexTable(RecordId,RecipeID)\n",
    "val recipeIndexTable = recordIndexes.join(recordIndexesOffset).flatMap(r=> (r._2._1 to r._2._2).map(t=>(t,r._1)))\n",
    "recipeIndexTable.collect.sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now join it to the imported data so we add the recipeID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcsvIndexed\u001b[0m: org.apache.spark.rdd.RDD[(Long, ((String, Boolean), Long))] = MapPartitionsRDD[21] at join at Main.scala:27\n",
       "\u001b[36mres5_1\u001b[0m: Array[(Long, ((String, Boolean), Long))] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m3558L\u001b[0m,\n",
       "    \u001b[33m\u001b[0m(\n",
       "      \u001b[33m\u001b[0m(\n",
       "        \u001b[32m\"Blend well the cream cheese with the Brie cheese. Add the hazelnuts and apple; blend. Spread on melba toast or crackers.\"\u001b[0m,\n",
       "        \u001b[32mfalse\u001b[0m\n",
       "      ),\n",
       "      \u001b[32m92L\u001b[0m\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1084L\u001b[0m, \u001b[33m\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"- - - - - - - - - - - - - - - - - - -\"\u001b[0m, \u001b[32mfalse\u001b[0m), \u001b[32m28L\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3586L\u001b[0m, \u001b[33m\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"2            tb  Olive oil\"\u001b[0m, \u001b[32mfalse\u001b[0m), \u001b[32m93L\u001b[0m)),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1410L\u001b[0m, \u001b[33m\u001b[0m(\u001b[33m\u001b[0m(\u001b[32m\"\"\u001b[0m, \u001b[32mfalse\u001b[0m), \u001b[32m37L\u001b[0m))\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val csvIndexed = csv.map(r=>(r._3,(r._1,r._2))).join(recipeIndexTable)\n",
    "csvIndexed.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine the recipe lines separated by a pipe (|) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.util.matching.Regex\u001b[0m\n",
       "\u001b[36mrecipesText\u001b[0m: org.apache.spark.rdd.RDD[(Long, String, String)] = MapPartitionsRDD[32] at map at Main.scala:36\n",
       "\u001b[36mres6_2\u001b[0m: Array[(Long, String, String)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m34L\u001b[0m,\n",
       "    \u001b[32m\"Cheese-Olive Balls\"\u001b[0m,\n",
       "    \u001b[32m\"\"\"\n",
       "1/4      teaspoon  hot pepper sauce    \n",
       "1      teaspoon  paprika    \n",
       "1/2      teaspoon  salt    \n",
       "2          cups  sharp cheddar cheese -- grated    \n",
       "1/2           cup  butter    \n",
       "1           cup  flour -- sifted    \n",
       "olives    \n",
       "    \n",
       "Mix ingredientsexcept olives  like a pie crust.  Wrap each olive with mixture.  spread the little balls on a pan and freeze.  Bake at 425 degrees for 12 minutes. Can be keep frozen in a bag. Serve hot.    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.matching.Regex\n",
    "\n",
    "\n",
    "val recipesText = csvIndexed\n",
    "    .map(r=>(r._2._2,r._1,r._2._1._1))// let's flatten the nested list so we have RecipeId, Recipe Line Id and Recipe Line\n",
    "    .sortBy(r=>(r._1,r._2)) // Properly sort it so we have all lines consecutively arranged as per the recipe line id\n",
    "    .map(r=>(r._1,r._3)) // retain only the recipeid and recipe lines\n",
    "    .groupBy(_._1) // and group it by the RecipeId\n",
    "    .map( \n",
    "        g => (\n",
    "                g._1,    // return the RecipeId\n",
    "                g._2.map(_._2.trim).mkString(\"\\n\")\n",
    "        )  // and concatenate the nested array of recipe lines with a pipe\n",
    "        )\n",
    "    .map { g=>\n",
    "        val pattern = new Regex(\"\"\"(?s)\\* Exported from MasterCook \\*\\n\\n([^\\n]+)\\n\\n\"\"\", \"RecipeName\")\n",
    "        (g._1,\n",
    "        pattern.findFirstMatchIn(g._2).get.group(\"RecipeName\"),\n",
    "        g._2.replaceAll(\"\"\"(?is)^.+---------\\n\"\"\",\"\").trim\n",
    "        )\n",
    "         }\n",
    "\n",
    "recipesText.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's extract our data in a long format: RecipeIngredients(RecipeName,RecipeNameHash, RecipeText) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecipeIngredients\u001b[0m: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[33] at map at Main.scala:26\n",
       "\u001b[36mres7_1\u001b[0m: Array[(String, String)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"Cheese-Olive Balls\"\u001b[0m,\n",
       "    \u001b[32m\"\"\"\n",
       "1/4      teaspoon  hot pepper sauce    \n",
       "1      teaspoon  paprika    \n",
       "1/2      teaspoon  salt    \n",
       "2          cups  sharp cheddar cheese -- grated    \n",
       "1/2           cup  butter    \n",
       "1           cup  flour -- sifted    \n",
       "olives    \n",
       "    \n",
       "Mix ingredientsexcept olives  like a pie crust.  Wrap each olive with mixture.  spread the little balls on a pan and freeze.  Bake at 425 degrees for 12 minutes. Can be keep frozen in a bag. Serve hot.    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "    \n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var recipeIngredients = recipesText\n",
    "    .map{\n",
    "        text=>\n",
    "            (text._2, text._3) // amd return (RecipeName, RecipeText)\n",
    "    }\n",
    "\n",
    "recipeIngredients.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark-ML Transformations Library\n",
    "\n",
    "In contrast to Sparl's MLLlib library, ML is much simpler and offers a simplified interface. http://spark.apache.org/docs/latest/ml-guide.html (Lot's of code has been borrowed from this site)\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "* Create a Dataframe\n",
    "* Pre-process the text: Remove unwanted chars, stop words and tokenize\n",
    "* Convert the TF-IDF (HashingTF and IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.functions._\u001b[0m\n",
       "\u001b[36msqlContext\u001b[0m: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5cd1fd15\n",
       "\u001b[36msentenceData\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string]\n",
       "\u001b[36msentenceDataClean\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string]\n",
       "\u001b[36mres8_5\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [ teaspoon hot pepper sauce teaspoon paprika teaspoon salt cups sharp cheddar cheese grated cup butter cup flour sifted olives mix ingredientsexcept olives like a pie crust wrap each olive with mixture spread the little balls on a pan and freeze bake at degrees for minutes can be keep frozen in a bag serve hot nutr assoc ],\n",
       "  [ ounces jumbo ripe olives canned pitted cup italian dressing bunch green onions drain olives and marinate at room temperature in dressing for one hour or more turning to coat on all sides cut green onions into one inch pieces slash one end of each piece to make a fringe soak onions in ice water while olives are marinating drain onions and olives stuff each olive with an onion fringed end sticking out yield serves to nutr assoc ],\n",
       "  [ cup granulated sugar cup water teaspoon cinnamon teaspoon nutmeg teaspoon cloves pound roasted peanuts boil sugar water and spices until syrup threads from a spoon f drop pound of peanuts into syrup stir until nuts are coated pour out on foil allow to cool store in an airtight container source virginia carolina peanuts s internet address http aboutpeanuts com yield pounds nutr assoc ]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "import org.apache.spark.sql.functions._\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val sentenceData = sqlContext.createDataFrame(recipeIngredients).toDF(\"recipeName\", \"recipeText\")\n",
    "val sentenceDataClean = sentenceData.withColumn(\"recipeTextClean\", \n",
    "                        regexp_replace(\n",
    "                            regexp_replace(\n",
    "                                lower(sentenceData(\"recipeText\")) // make everything lowercase\n",
    "                                ,\"[^a-z]\",\" \" // replace non-letters for whitespaces\n",
    "                            )\n",
    "                            ,\" +\",\" \" // convert multiple whitespaces into a single space\n",
    "                        )\n",
    "                       )\n",
    "sentenceDataClean.select(\"recipeTextClean\").take(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tokenize the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[0m: org.apache.spark.ml.feature.Tokenizer = tok_85cef01bf337\n",
       "\u001b[36mtokenizedData\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string, words: array<string>]\n",
       "\u001b[36mres9_2\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [WrappedArray(, teaspoon, hot, pepper, sauce, teaspoon, paprika, teaspoon, salt, cups, sharp, cheddar, cheese, grated, cup, butter, cup, flour, sifted, olives, mix, ingredientsexcept, olives, like, a, pie, crust, wrap, each, olive, with, mixture, spread, the, little, balls, on, a, pan, and, freeze, bake, at, degrees, for, minutes, can, be, keep, frozen, in, a, bag, serve, hot, nutr, assoc)],\n",
       "  [WrappedArray(, ounces, jumbo, ripe, olives, canned, pitted, cup, italian, dressing, bunch, green, onions, drain, olives, and, marinate, at, room, temperature, in, dressing, for, one, hour, or, more, turning, to, coat, on, all, sides, cut, green, onions, into, one, inch, pieces, slash, one, end, of, each, piece, to, make, a, fringe, soak, onions, in, ice, water, while, olives, are, marinating, drain, onions, and, olives, stuff, each, olive, with, an, onion, fringed, end, sticking, out, yield, serves, to, nutr, assoc)],\n",
       "  [WrappedArray(, cup, granulated, sugar, cup, water, teaspoon, cinnamon, teaspoon, nutmeg, teaspoon, cloves, pound, roasted, peanuts, boil, sugar, water, and, spices, until, syrup, threads, from, a, spoon, f, drop, pound, of, peanuts, into, syrup, stir, until, nuts, are, coated, pour, out, on, foil, allow, to, cool, store, in, an, airtight, container, source, virginia, carolina, peanuts, s, internet, address, http, aboutpeanuts, com, yield, pounds, nutr, assoc)]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// tokenize\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"recipeTextClean\").setOutputCol(\"words\")\n",
    "val tokenizedData = tokenizer.transform(sentenceDataClean)\n",
    "\n",
    "\n",
    "tokenizedData.select(\"words\").take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.StopWordsRemover\u001b[0m\n",
       "\u001b[36mremover\u001b[0m: org.apache.spark.ml.feature.StopWordsRemover = stopWords_f95bb8207aab\n",
       "\u001b[36mres10_2\u001b[0m: org.apache.spark.ml.feature.StopWordsRemover = stopWords_f95bb8207aab\n",
       "\u001b[36mswdData\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string, words: array<string>, filtered: array<string>]\n",
       "\u001b[36mres10_4\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [WrappedArray(, teaspoon, hot, pepper, sauce, teaspoon, paprika, teaspoon, salt, sharp, cheddar, cheese, grated, cup, cup, flour, sifted, olives, mix, ingredientsexcept, olives, like, pie, crust, wrap, olive, mixture, spread, little, balls, pan, freeze, bake, degrees, minutes, frozen, bag, serve, hot)],\n",
       "  [WrappedArray(, ounces, jumbo, ripe, olives, canned, pitted, cup, italian, dressing, bunch, green, onions, drain, olives, marinate, room, temperature, dressing, hour, turning, coat, sides, cut, green, onions, inch, pieces, slash, end, piece, fringe, soak, onions, ice, water, olives, marinating, drain, onions, olives, stuff, olive, onion, fringed, end, sticking, serves)],\n",
       "  [WrappedArray(, cup, granulated, sugar, cup, water, teaspoon, cinnamon, teaspoon, nutmeg, teaspoon, cloves, pound, boil, sugar, water, spices, syrup, threads, spoon, f, drop, pound, syrup, stir, nuts, coated, pour, foil, allow, cool, store, airtight, container, virginia, carolina, s, aboutpeanuts, pounds)]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// remove stop words\n",
    "\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"filtered\")\n",
    "remover.setStopWords(remover.getStopWords++Array(\"exported\",\"internet\",\"address\",\"com\",\"from\",\"mastercook\",\"recipe\",\"by\",\"serving\",\"size\",\"preparation\",\"time\",\"categories\",\"amount\",\"measure\",\"ingredient\",\"preparation\",\"method\",\"place\",\"all\",\"ingredients\",\"in\",\"copyright\",\"notice\",\"taken\",\"raw\",\"gourmet\",\"simple\",\"recipes\",\"living\",\"nomi\",\"shannon\",\"nomi\",\"shannon\",\"commercial\",\"rights\",\"reserved\",\"distributed\",\"freely\",\"non\",\"commercial\",\"purposes\",\"provided\",\"copyright\",\"notice\",\"included\",\"following\",\"web\",\"site\",\"http\",\"www\",\"living\",\"foods\",\"rawgourmet\",\"contact\",\"author\",\"questions\",\"regarding\",\"matter\",\"rawgourmet\",\"living\",\"foods\",\"source\",\"http\",\"www\",\"living\",\"foods\",\"recipes\",\"gadogado\",\"html\",\"copyright\",\"nomi\",\"shannon\",\"read\",\"copyright\",\"notice\",\"yield\",\"cups\",\"notes\",\"based\",\"indonesian\",\"dish\",\"traditionally\",\"peanuts\",\"using\",\"almonds\",\"peanuts\",\"recommended\",\"fungus\",\"called\",\"aflatoxin\",\"naturally\",\"occurs\",\"peanut\",\"crop\",\"crops\",\"inspected\",\"certain\",\"percentage\",\"allowed\",\"proven\",\"carcinogen\",\"peanuts\",\"best\",\"left\",\"reason\",\"peanut\",\"butter\",\"isn\",\"t\",\"recommended\",\"possible\",\"make\",\"butter\",\"raw\",\"peanuts\",\"peanut\",\"butter\",\"produced\",\"roasted\",\"peanuts\",\"nutr\",\"assoc\"))\n",
    "val swdData = remover.transform(tokenizedData)\n",
    "\n",
    "swdData.select(\"filtered\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Failure(End:1:66 ...\"with the h\")"
     ]
    }
   ],
   "source": [
    "Now we can use the limited list of words to generate a dataframe with the hashed features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mhashingTF\u001b[0m: org.apache.spark.ml.feature.HashingTF = hashingTF_0098bbfdaeb2\n",
       "\u001b[36mfeaturizedData\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string, words: array<string>, filtered: array<string>, rawFeatures: vector]\n",
       "\u001b[36mres11_2\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [(500,[0,1,23,36,42,85,93,119,124,146,153,160,163,206,249,251,263,279,284,288,302,335,349,355,371,378,398,423,432,455,482,488],[1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0])],\n",
       "  [(500,[0,8,26,59,71,75,85,93,97,103,138,139,141,152,171,175,176,229,236,250,252,284,297,316,317,356,372,378,382,395,423,432,438,443,447,488,494,496],[1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0])],\n",
       "  [(500,[0,4,10,17,29,51,52,66,93,100,102,115,129,158,176,207,216,229,309,310,313,329,361,366,378,418,419,447,469,477],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0])]\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// hash-tf array\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "  .setInputCol(\"filtered\").setOutputCol(\"rawFeatures\").setNumFeatures(500)\n",
    "val featurizedData = hashingTF.transform(swdData)\n",
    "\n",
    "featurizedData.select(\"rawFeatures\").take(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36midf\u001b[0m: org.apache.spark.ml.feature.IDF = idf_8d7f8f85eb7e\n",
       "\u001b[36midfModel\u001b[0m: org.apache.spark.ml.feature.IDFModel = idf_8d7f8f85eb7e\n",
       "\u001b[36mrescaledData\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string, words: array<string>, filtered: array<string>, rawFeatures: vector, features: vector]\n",
       "\u001b[36mres12_3\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [(500,[0,1,23,36,42,85,93,119,124,146,153,160,163,206,249,251,263,279,284,288,302,335,349,355,371,378,398,423,432,455,482,488],[0.059423420470800806,2.295766675349785,1.6486586255873816,1.1478833376748925,1.8718021769015913,1.5998684614179497,1.5037685182495197,0.9067212808580044,0.9710156315634016,0.6931471805599453,1.1180303745252111,2.159484249353372,1.3121863889661687,1.0608719606852626,1.5051412020614923,2.5649493574615367,1.2770950691548986,1.754019141245208,1.754019141245208,1.9363406980391626,1.8718021769015913,0.6013396313068224,2.005333569526114,0.7125652664170469,2.341805806147327,0.4753433037542391,2.4471663218051534,1.5998684614179497,3.7436043538031827,1.1478833376748925,1.754019141245208,2.0794415416798357])],\n",
       "  [(500,[0,8,26,59,71,75,85,93,97,103,138,139,141,152,171,175,176,229,236,250,252,284,297,316,317,356,372,378,382,395,423,432,438,443,447,488,494,496],[0.059423420470800806,2.4471663218051534,7.487208707606365,3.258096538021482,2.772588722239781,2.8526314299133175,1.5998684614179497,0.5012561727498399,1.754019141245208,1.5533484457830569,2.005333569526114,3.7436043538031827,1.5533484457830569,3.0349529867072724,2.2464956263430023,2.4471663218051534,2.341805806147327,2.0794415416798357,2.6984807500860595,2.8526314299133175,2.6984807500860595,1.754019141245208,1.1180303745252111,2.159484249353372,0.9555114450274363,2.932674137586854,2.341805806147327,0.23767165187711956,0.9067212808580044,1.5998684614179497,1.599\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// rescale wordcounts\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "\n",
    "rescaledData.select(\"features\").take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's estimate a good number of features for the PCA by using the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\u001b[0m\n",
       "\u001b[36mvariances\u001b[0m: Array[Double] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m6.017903085109311\u001b[0m,\n",
       "  \u001b[32m4.757474156511935\u001b[0m,\n",
       "  \u001b[32m4.740411822959413\u001b[0m,\n",
       "  \u001b[32m4.4541309892818886\u001b[0m,\n",
       "  \u001b[32m3.335603581688776\u001b[0m,\n",
       "  \u001b[32m3.1993890370482068\u001b[0m,\n",
       "  \u001b[32m3.186828120085474\u001b[0m,\n",
       "  \u001b[32m3.054532046090568\u001b[0m,\n",
       "  \u001b[32m3.0070157507307647\u001b[0m,\n",
       "  \u001b[32m2.992070101863424\u001b[0m,\n",
       "  \u001b[32m2.9445148418187963\u001b[0m,\n",
       "  \u001b[32m2.5951491059948553\u001b[0m,\n",
       "  \u001b[32m2.477303977401982\u001b[0m,\n",
       "  \u001b[32m2.425215832796767\u001b[0m,\n",
       "  \u001b[32m2.405393024411766\u001b[0m,\n",
       "  \u001b[32m2.393122497879171\u001b[0m,\n",
       "  \u001b[32m2.367174662584539\u001b[0m,\n",
       "  \u001b[32m2.3385269239482094\u001b[0m,\n",
       "  \u001b[32m2.2817463683701344\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mvarianceTotal\u001b[0m: Double = \u001b[32m374.47878621576535\u001b[0m\n",
       "\u001b[36mk\u001b[0m: Int = \u001b[32m243\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Estimate the k parameter for PCA based on the number of features that explain up to 80% of variance\n",
    "\n",
    "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n",
    "// calculate the column variances\n",
    "val variances = new RowMatrix(rescaledData.select(\"features\").rdd.map(r => r(0).asInstanceOf[org.apache.spark.mllib.linalg.SparseVector])).computeColumnSummaryStatistics.variance.toArray.sorted(Ordering[Double].reverse)\n",
    "// and the total variance\n",
    "val varianceTotal = variances.reduceLeft((a,b) => a + b)\n",
    "// get the number of columns that explain 80% of the variance and use this as k\n",
    "val k = variances.map{var s = 0.0; d => {s += d; s/varianceTotal}}.zipWithIndex.filter(_._1>0.8).head._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's run the PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.PCA\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.Vectors\u001b[0m\n",
       "\u001b[36mpca\u001b[0m: org.apache.spark.ml.feature.PCAModel = pca_02c2194e6894\n",
       "\u001b[36mpcaDF\u001b[0m: org.apache.spark.sql.DataFrame = [recipeName: string, recipeText: string, recipeTextClean: string, words: array<string>, filtered: array<string>, rawFeatures: vector, features: vector, pcaFeatures: vector]\n",
       "\u001b[36mres14_4\u001b[0m: Array[org.apache.spark.sql.Row] = \u001b[33mArray\u001b[0m(\n",
       "  [Cheese-Olive Balls,[-1.5745572325731925,0.22192109724859044,-0.1580542136807957,-0.3123119779488005,0.1517057539806714,-1.6133968878431084,-0.07801142995610842,0.1551214441506181,-0.18823113908511294,-1.0957153773622479,-0.40657782344279053,-0.28137855784953036,0.49419586340560806,-0.033542182171615434,-0.6924852729615883,-0.054834622388484106,0.34504507374813814,0.8023428955688754,-0.03803106861617622,-1.1144731966830235,-0.06428218365788038,-0.7269873333506213,-0.22923701170976507,1.3416618107695653,-0.04780765535642029,-0.35852080580113055,0.45785491454729044,-1.5349587729533245,-0.8435143163523342,-0.5386433510625745,-0.43761212604626887,-0.2680017217616157,0.7811618805953637,-1.1263539241632283,0.18199984653067997,0.8692473509735538,-0.46269103982396365,0.28175522518443097,-0.0925910637245031,-0.9605639754746693,-0.026737259907121884,0.3101976936809877,-0.7007323481534451,0.31161509986236696,2.048598801769689,0.09570253465698227,0.2432172375930208,1.6391061830911868,0.5633501771177006,0.456729281340037,0.31759490225093545,-0.1286265833740666,0.9728441899250981,2.2711576443480457,-0.6417278296326301,0.5213854514071081,0.2653714703298089,1.010628988624286,1.4882180295383471,0.10491397162659673,-0.6620637551926097,0.8263156419840019,-0.697538610964668,0.0766525733512145,0.030017788853427874,0.14997688561785144,-0.7713564866525003,1.052278595324519,-0.35184215265599306,-0.3058167964767246,0.28966518417205295,0.8759890684362945,-0.14843569436480097,0.8486874411657679,0.4321836462820265,0.0819\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/// PCA\n",
    "\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(k)\n",
    "  .fit(rescaledData)\n",
    "val pcaDF = pca.transform(rescaledData)\n",
    "pcaDF.select(\"recipeName\",\"pcaFeatures\").take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the text-based Recipe-Recipe Cosine Simmilarity Model\n",
    "\n",
    "In order to calculate the cosine simmilarities, we need to implement the formula as one is not available in the Dataframe object. \n",
    "\n",
    "The goal is to calculate the recipe-recipe cosine similarities based on the PCA-derived features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecipePcaRdd\u001b[0m: Array[(Long, String, Array[Double], Double, String)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m0L\u001b[0m,\n",
       "    \u001b[32m\"Cheese-Olive Balls\"\u001b[0m,\n",
       "    \u001b[33mArray\u001b[0m(\n",
       "      \u001b[32m-1.5745572325731925\u001b[0m,\n",
       "      \u001b[32m0.22192109724859044\u001b[0m,\n",
       "      \u001b[32m-0.1580542136807957\u001b[0m,\n",
       "      \u001b[32m-0.3123119779488005\u001b[0m,\n",
       "      \u001b[32m0.1517057539806714\u001b[0m,\n",
       "      \u001b[32m-1.6133968878431084\u001b[0m,\n",
       "      \u001b[32m-0.07801142995610842\u001b[0m,\n",
       "      \u001b[32m0.1551214441506181\u001b[0m,\n",
       "      \u001b[32m-0.18823113908511294\u001b[0m,\n",
       "      \u001b[32m-1.0957153773622479\u001b[0m,\n",
       "      \u001b[32m-0.40657782344279053\u001b[0m,\n",
       "      \u001b[32m-0.28137855784953036\u001b[0m,\n",
       "      \u001b[32m0.49419586340560806\u001b[0m,\n",
       "      \u001b[32m-0.033542182171615434\u001b[0m,\n",
       "      \u001b[32m-0.6924852729615883\u001b[0m,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val recipePcaRdd = pcaDF\n",
    "    .select(\"recipeName\",\"pcaFeatures\",\"filtered\")\n",
    "    .rdd\n",
    "    .zipWithIndex\n",
    "    .collect\n",
    "    .map(r=> (r._2,\n",
    "              r._1(0).toString,\n",
    "              r._1(1).asInstanceOf[org.apache.spark.mllib.linalg.DenseVector].toArray,\n",
    "              Math.sqrt(r._1(1)\n",
    "                        .asInstanceOf[org.apache.spark.mllib.linalg.DenseVector]\n",
    "                        .toArray\n",
    "                        .reduce((T,v) => T + v*v)\n",
    "                       ), // calculate the vector length\n",
    "              r._1(2).toString\n",
    "             )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msimmilarities\u001b[0m: Array[(String, String, Double, String, String)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"Blue Cheese Stuffed Mushrooms\"\u001b[0m,\n",
       "    \u001b[32m\"Stuffed Mushrooms\"\u001b[0m,\n",
       "    \u001b[32m0.6884983721232661\u001b[0m,\n",
       "    \u001b[32m\"WrappedArray(, large, fresh, mushrooms, tablespoons, margarine, cup, finely, chopped, red, pepper, cup, heavy, cream, cup, crumbled, blue, cheese, cooked, rice, tablespoon, minced, fresh, basil, teaspoon, ground, white, pepper, fresh, basil, chopped, garnish, clean, mushrooms, damp, paper, towel, remove, mushroom, stems, finely, chop, stems, set, aside, saute, mushroom, caps, skillet, tender, drain, paper, towels, saute, mushroom, stems, red, pepper, skillet, add, cream, bring, boil, reduce, heat, add, cheese, cook, melted, stir, rice, basil, pepper, cook, thoroughly, heated, spoon, rice, mixture, mushroom, caps, mushroom, caps, greased, shallow, baking, pan, cover, bake, degrees, minutes, tender, drain, paper, towels, garnish, stuffed, mushrooms, basil, rice, council, s, usarice)\"\u001b[0m,\n",
       "    \u001b[32m\"WrappedArray(, mushrooms, medium, tablespoons, margarine, cup, onion, chopped, medium, tablespoons, white, wine, dry, cup, bread, crumbs, dry, cup, cooked, smoked, ham, fine, chop, tablespoons, parsley, snipped, tablespoon, lime, juice, clove, garlic, finely, chopped, teaspoon, oregano, leaves, dried, dash, pepper, cup, monterey, jack, cheese, shredded, cut, stems, mushrooms, finely, chop, st\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val simmilarities = recipePcaRdd.flatMap(r0=> recipePcaRdd\n",
    "                     .filter(r1 => r1._1>r0._1)\n",
    "                     .map(r1 => \n",
    "                          (r0._2,\n",
    "                            r1._2,\n",
    "                            (0 to r1._3.length-1).map(i=>r0._3(i)*r1._3(i)).foldLeft(0.0)((T,v) => T + v)/(r0._4*r1._4),\n",
    "                            r0._5,\n",
    "                            r1._5\n",
    "                           )\n",
    "                    )\n",
    "    ).sortBy(-_._3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mdisplayTable\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Helper function that displays a nicely formatted table\n",
    "def displayTable(table:List[Map[String, String]])(implicit publish: jupyter.api.Publish[jupyter.api.Evidence]): Unit = {\n",
    "    val keys = table.flatMap(r=>r.keys).distinct.sorted\n",
    "    val header = \"<th>\"+keys.mkString(\"</th><th>\")+\"</th>\"\n",
    "    val rows = \"<tr>\"+table.map(r=>keys.map(k=>\"<td>\"+r.getOrElse(k,\"&nbsp;\")+\"</td>\")).mkString(\"</tr><tr>\")+\"</tr>\"\n",
    "    publish.display(\"table\",(\"text/html\" -> (\"<table>\"+header+rows+\"</table>\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recipe-Recipe Simliarity Based Model\n",
    "\n",
    "Let's see the top-20 similar reciples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><th>Cosine Similarity</th><th>Recipe 0</th><th>Recipe 1</th><tr>List(<td>0.6884983721232661</td>, <td>Blue Cheese Stuffed Mushrooms</td>, <td>Stuffed Mushrooms</td>)</tr><tr>List(<td>0.6702883263140217</td>, <td>Mushrooms Filled with Feta Cheese and Pine Nuts</td>, <td>Blue Cheese Stuffed Mushrooms</td>)</tr><tr>List(<td>0.6415835534206843</td>, <td>Mushrooms Filled with Feta Cheese and Pine Nuts</td>, <td>Stuffed Mushrooms</td>)</tr><tr>List(<td>0.6112733989136132</td>, <td>Five-Spice Appetizer Meatballs</td>, <td>Sweet and Sour Party Meat Balls</td>)</tr><tr>List(<td>0.5367300021737985</td>, <td>Pork Meatball With Sweet-Sour Sauce, Pk</td>, <td>Five-Spice Appetizer Meatballs</td>)</tr><tr>List(<td>0.5139370241898292</td>, <td>Fruit Salsa Dip</td>, <td>Ants on a Log</td>)</tr><tr>List(<td>0.4912639950927898</td>, <td>Deviled Eggs</td>, <td>Deviled Egg Slices, Pk</td>)</tr><tr>List(<td>0.47639903375738374</td>, <td>Elegant Vegetarian Pate</td>, <td>Mushroom Individuals</td>)</tr><tr>List(<td>0.47309492037982354</td>, <td>Cheesy Wontons With Sweet and Sour Dip</td>, <td>Ginger-Date Wontons</td>)</tr><tr>List(<td>0.47138763032470227</td>, <td>Southwestern Chicken Filo Triangles</td>, <td>Italian Roasted Vegetables</td>)</tr><tr>List(<td>0.46919087456015435</td>, <td>Pot Stickers</td>, <td>Southwestern Chicken Filo Triangles</td>)</tr><tr>List(<td>0.459574980895808</td>, <td>Rice Krispies Balls, Pk</td>, <td>Ella's Divine Date Rum Balls</td>)</tr><tr>List(<td>0.4558878509365811</td>, <td>Orange Coconut Balls</td>, <td>Cranberry Coconut Fruit Balls</td>)</tr><tr>List(<td>0.45378346544793907</td>, <td>Snack Sandwiches</td>, <td>Cucumber Rye Surprises, Pk</td>)</tr><tr>List(<td>0.4528849669076496</td>, <td>Orange Coconut Balls</td>, <td>Crunchy Chocolate-Coconut Balls</td>)</tr><tr>List(<td>0.44236882938978195</td>, <td>Shirlie's Cheese Ball</td>, <td>Cheese Ball</td>)</tr><tr>List(<td>0.436953776616695</td>, <td>Yellow And Red Bell Peppers Filled With Tuna</td>, <td>Italian Roasted Vegetables</td>)</tr><tr>List(<td>0.4169434373342464</td>, <td>Pork Meatball With Sweet-Sour Sauce, Pk</td>, <td>Sweet and Sour Party Meat Balls</td>)</tr><tr>List(<td>0.41517227492464187</td>, <td>Deviled Egg Slices, Pk</td>, <td>Devilish Eggs</td>)</tr><tr>List(<td>0.4134202519029731</td>, <td>Southwestern Chicken Filo Triangles</td>, <td>Pineapple Chicken Satay</td>)</tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayTable(simmilarities\n",
    "//             .filter(_._3 >0.5)\n",
    "             .map( r=>\n",
    "                        Map(\"Recipe 0\" -> r._1,\n",
    "                            \"Recipe 1\" -> r._2,\n",
    "                            \"Cosine Similarity\" -> r._3.toString//,\n",
    "//                            \"Recipe Text 0\" -> r._4,\n",
    "//                            \"Recipe Text 1\" -> r._5\n",
    "                           )\n",
    "                )\n",
    "             .toList\n",
    "             .take(20)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting it:\n",
    "\n",
    "Scala/Spark does not offer much plotting options. For convenience, let's embeed a static plotly graph. I'll eventually figure out how to dynamically pass data to the graph from my app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"https://plot.ly/~rmalarc/5.embed\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "publish.display(\"table\",(\"text/html\" -> (\"\"\"<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"https://plot.ly/~rmalarc/5.embed\"></iframe>\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "* I'm getting better with Spark!.\n",
    "* The ML library is much user-friendly than ML-LIB, even though it's not as feature-rich. \n",
    "* The PCA-based relationships appear to work, although it's hard to prove as I'm not that familiar with the recipes. I suspect that there is a fair amount of noise due to certain keywords which should be added to the stop-word list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
