{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Collaborative Filtering Recommender System for Low-Latency Document Classification Engine\n",
    "\n",
    "*Mauricio Alarcon <rmalarc@msn.com>*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Traditional document classification systems are based on machine learning algorithms such as logistic regression, naive bayes classification, amongst others. These supervised algorithms require an extensive dataset before they can start doing their job.\n",
    "\n",
    "What if you have an interactive application where you need to generate a doucument classification based upon limited user interaction and there is no prior training dataset? \n",
    "\n",
    "We could use one of the traditional classification algorithms in a way that we first generate a training datased by capturing several records of user interaction and then generate predictions. This system adds latency, as the system would not be able to generate predictions until a rich training dataset is first generated. This latency often makes these algorithms hard and impractical to implement due to the dependency on the existence of a rich training dataset.\n",
    "\n",
    "In this project we are developing a low-latency document-document recommender system by generating a prospective lean training dataset captured from user interaction that minimizes prediction latency.\n",
    "\n",
    "In reality, I'm working on a real application that needs documment classification that works more in a \"streaming\" fashion.\n",
    "\n",
    "## The Application Workflow\n",
    "\n",
    "The overal workflow of the application (and how it relates to the classification engine) is as follows:\n",
    "\n",
    "1. User provides the URL of a doucment\n",
    "2. The sytem featurizes and calculates the cosine simmilarity against any one of the existing documents in the training dataset (if any) leading to the following fork:\n",
    "  1. A similar document was found in the training data: The document category is then inherited to the current document and presented to the user. If the user then overrides the classification, we consider this as a new document category and the doucment is added to the training dataset along with the user-provided category\n",
    "  2. No similar documents were found: The user is prompted for the document category and the data gets added to the training dataset.\n",
    "\n",
    "Although the results of this classification are visible to the user, the main intended beneficiary of the engine is the application itself, as based in the document type the execution flow of the app can then be forked based on the output. \n",
    "\n",
    "## Scope and Deliverables\n",
    "\n",
    "In alignment with the course I intend to break this project in two installments as follows:\n",
    "\n",
    "### Project IV\n",
    "\n",
    "* The document classification engine\n",
    "* A light-weight SBT console-based application for testing\n",
    "\n",
    "### Final Project\n",
    "\n",
    "I have two options:\n",
    "\n",
    "* In real life, I will integrate the described engine as part of a proprietary application that extracts information from documents. However, due to the properietary nature and vast codebase I cannot make the code of this application avaiable. I could however do a demonstration and show the final application functionality.\n",
    "\n",
    "* Alternatively I could generate a mock web-based application I intend to integrate the engine in some type of web-based application.\n",
    "\n",
    "Let me know which of the two options is acceptable to you\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 new artifact(s)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160 new artifacts in macro\n",
      "160 new artifacts in runtime\n",
      "160 new artifacts in compile\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add( \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\",\n",
    "              \"org.apache.spark\" %% \"spark-mllib\" % \"1.6.1\",\n",
    "              \"org.apache.spark\" %% \"spark-sql\" % \"1.6.1\",\n",
    "              \"net.htmlparser.jericho\" % \"jericho-html\" % \"3.3\",\n",
    "              \"org.jsoup\" % \"jsoup\" % \"1.9.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "## The Recommender System\n",
    "\n",
    "This is a cosine-similarity based collaborative filtering document classifier.\n",
    "\n",
    "## The Code\n",
    "\n",
    "### Firing up a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SQLContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.{SparkConf, SparkContext}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mSparkEngine\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "// let's define the spark engine as it's own object that will be extended to result into the document classifier \n",
    "object SparkEngine{\n",
    "  var sc: SparkContext = new SparkContext(\n",
    "    new SparkConf()\n",
    "      .setAppName(\"Datasiv\")\n",
    "      .setMaster(\"local[2]\")\n",
    "  )\n",
    "  val sqlContext = new SQLContext(sc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/07/10 14:02:29 INFO SparkContext: Running Spark version 1.6.1\n",
      "16/07/10 14:02:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/07/10 14:02:29 INFO SecurityManager: Changing view acls to: malarconba001\n",
      "16/07/10 14:02:29 INFO SecurityManager: Changing modify acls to: malarconba001\n",
      "16/07/10 14:02:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(malarconba001); users with modify permissions: Set(malarconba001)\n",
      "16/07/10 14:02:30 INFO Utils: Successfully started service 'sparkDriver' on port 8384.\n",
      "16/07/10 14:02:31 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/07/10 14:02:31 INFO Remoting: Starting remoting\n",
      "16/07/10 14:02:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.0.4:8397]\n",
      "16/07/10 14:02:31 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 8397.\n",
      "16/07/10 14:02:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/07/10 14:02:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/07/10 14:02:31 INFO DiskBlockManager: Created local directory at C:\\Users\\malarconba001\\AppData\\Local\\Temp\\blockmgr-ce8b34d2-dae0-434e-9a1f-d3e546e90bba\n",
      "16/07/10 14:02:31 INFO MemoryStore: MemoryStore started with capacity 1773.8 MB\n",
      "16/07/10 14:02:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/07/10 14:02:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/07/10 14:02:32 INFO SparkUI: Started SparkUI at http://192.168.0.4:4040\n",
      "16/07/10 14:02:32 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/07/10 14:02:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8408.\n",
      "16/07/10 14:02:32 INFO NettyBlockTransferService: Server created on 8408\n",
      "16/07/10 14:02:32 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/07/10 14:02:32 INFO BlockManagerMasterEndpoint: Registering block manager localhost:8408 with 1773.8 MB RAM, BlockManagerId(driver, localhost, 8408)\n",
      "16/07/10 14:02:32 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2\u001b[0m: \u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@141b6656"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SparkEngine.sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the spark engine defined, let's create the DocumentClassifier engine with the followig methods and attributes:\n",
    "\n",
    "* documentClassifierTrainingData: It's a list of documentCategories and documentText\n",
    "* appendDocumentCategory: It appends to the trainingdata\n",
    "* clearDocumentCategories: Resets the training data\n",
    "* classifyDocument: Takes a documentText and returns the cosine similarity with the existing categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.feature.{HashingTF,PCA,IDF, Normalizer, StopWordsRemover, Tokenizer}\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.linalg.SparseVector\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.{Pipeline, PipelineModel}\u001b[0m\n",
       "defined \u001b[32mobject \u001b[36mDocumentClassifier\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF,PCA,IDF, Normalizer, StopWordsRemover, Tokenizer}\n",
    "import org.apache.spark.mllib.linalg.SparseVector\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "\n",
    "//  This is the docuemnt Classifier engine which contains the following attributes and methods:\n",
    "// * documentClassifierTrainingData: It's a list of documentCategories and documentText\n",
    "// * appendDocumentCategory: It appends to the trainingdata\n",
    "// * clearDocumentCategories: Resets the training data\n",
    "// * classifyDocument: Takes a documentText and returns the cosine similarity with the existing categories\n",
    "object DocumentClassifier{\n",
    "  import SparkEngine._\n",
    "  var documentClassifierTrainingData: List[(String, String)] = List()\n",
    "\n",
    "  def appendDocumentCategory(docType:String, docText: String) = {\n",
    "    documentClassifierTrainingData = documentClassifierTrainingData ::: List(\n",
    "          (docType,\n",
    "            docText.toLowerCase.replaceAll(\"(?is)[^a-z]\",\" \").replaceAll(\"(?is) +\",\" \")\n",
    "            )\n",
    "        )\n",
    "  }\n",
    "\n",
    "  def clearDocumentCategories = {\n",
    "    documentClassifierTrainingData = List()\n",
    "  }\n",
    "\n",
    "\n",
    "  def classifyDocument(docText: String) = {\n",
    "    val predictData = List(\n",
    "      (\"CLASSIFYME\",\n",
    "        docText\n",
    "        )\n",
    "    )\n",
    "\n",
    "    // get all training data and create a dataframe out of it\n",
    "    val allData = sqlContext.createDataFrame(documentClassifierTrainingData ::: predictData).toDF(\"docType\", \"docText\")\n",
    "\n",
    "    // Let's configure the pipeline stages: Tokenizer, Stop WordsRemover, HashingTf with 500 features, IDF normalization and PCA with \n",
    "    val tokenizer = new Tokenizer()\n",
    "      .setInputCol(\"docText\")\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "    val remover = new StopWordsRemover()\n",
    "      .setInputCol(tokenizer.getOutputCol)\n",
    "      .setOutputCol(\"filtered\")\n",
    "\n",
    "    val hashingTF = new HashingTF()\n",
    "      .setNumFeatures(500) \n",
    "      .setInputCol(remover.getOutputCol)\n",
    "      .setOutputCol(\"hashed\")\n",
    "\n",
    "    val idf = new IDF()\n",
    "      .setInputCol(hashingTF.getOutputCol)\n",
    "      .setOutputCol(\"idfFeatures\")\n",
    "\n",
    "    val pca = new PCA()\n",
    "      .setInputCol(idf.getOutputCol)\n",
    "      .setOutputCol(\"features\")\n",
    "      .setK(350) // parameter value selection comes from last project\n",
    "\n",
    "\n",
    "    // LEt's now use the SparkML Pipeline in order to chain the transformations\n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(tokenizer, remover, hashingTF, idf, pca))\n",
    "\n",
    "    // Fit the pipeline to training documents.\n",
    "    val model = pipeline.fit(allData)\n",
    "      \n",
    "    // and get the output data\n",
    "    val hashedData = model\n",
    "      .transform(allData)\n",
    "      .select(\"docType\", \"features\")\n",
    "      \n",
    "    // with the output features, let's now separate the unknown document from the training documents. The goal is to calculate\n",
    "    // the cosine simmilarity of the new document against every other featurized document in the training dataset\n",
    "      \n",
    "    val unknownDocument = hashedData.filter(\"\"\"docType =\"CLASSIFYME\" \"\"\").collect.map(r => (r.getAs[String](\"docType\"), r.getAs[SparseVector](\"features\").toDense.toArray))\n",
    "    val trainingDocuments = hashedData.filter(\"\"\"docType <> \"CLASSIFYME\" \"\"\").collect.map(r => (r.getAs[String](\"docType\"), r.getAs[SparseVector](\"features\").toDense.toArray))\n",
    "\n",
    "    // The stuff below calculates the cosine similarities between the unknown document and each of the documents in the training dataset\n",
    "    val unknownDocumentsWithLengths = unknownDocument map {\n",
    "      td =>\n",
    "        (td._1,\n",
    "          td._2,\n",
    "          Math.sqrt(td._2.foldLeft(0.0)((T, r) => T + r * r))\n",
    "          )\n",
    "    }\n",
    "    val documentsWithLengths = trainingDocuments map {\n",
    "      td =>\n",
    "        (td._1,\n",
    "          td._2,\n",
    "          Math.sqrt(td._2.foldLeft(0.0)((T, r) => T + r * r))\n",
    "          )\n",
    "    }\n",
    "    val distances = documentsWithLengths map {\n",
    "      td =>\n",
    "        (td._1,\n",
    "          (0 to td._2.length - 1).map(i => unknownDocumentsWithLengths.head._2(i) * td._2(i))\n",
    "            .reduce((T, v) => T+v)\n",
    "            / (td._3 * unknownDocumentsWithLengths.head._3)\n",
    "          )\n",
    "    }\n",
    "    distances.sortBy(-_._2)\n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Document Classification Test\n",
    "\n",
    "Let's define an arbitrary couple of document categories. The idea is to compare document against only this training data with two elements in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//val simpleDocumentClassifier = new DocumentClassifier\n",
    "\n",
    "DocumentClassifier.appendDocumentCategory(\"houses\", \"the housing market is good\")\n",
    "DocumentClassifier.appendDocumentCategory(\"houses\", \"house repairs are time consuming\")\n",
    "DocumentClassifier.appendDocumentCategory(\"cars\",\"I have a green car\")\n",
    "DocumentClassifier.appendDocumentCategory(\"cars\",\"my car needs an oil change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5_0\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m0.3722732754300621\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m0.16479656365533427\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m-0.11308319283333286\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m-0.12075383616144986\u001b[0m)\n",
       ")\n",
       "\u001b[36mres5_1\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m0.9999999999999998\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m0.05447831503718207\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m-0.05288159202550208\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m-0.06124799298560581\u001b[0m)\n",
       ")\n",
       "\u001b[36mres5_2\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m-0.07865932426714198\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m-0.0859430585522185\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"houses\"\u001b[0m, \u001b[32m-0.09204264789908627\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"cars\"\u001b[0m, \u001b[32m-0.14400779823589419\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "DocumentClassifier.classifyDocument(\"green house\")\n",
    "DocumentClassifier.classifyDocument(\"I have a green car\")\n",
    "DocumentClassifier.classifyDocument(\"Trump of hilary? who knows who will win\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DocumentClassifier.clearDocumentCategories\n",
    "//SparkEngine.sc.stop\n",
    "//val dc2 = new DocumentClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Document Type Recommender with Wikipedia Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.jsoup\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.jsoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now join it to the imported data so we add the recipeID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DocumentClassifier.appendDocumentCategory(\"US States\", \n",
    "                                          jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States\").get.body.text\n",
    "                                         )\n",
    "\n",
    "DocumentClassifier.appendDocumentCategory(\"Technology Manufacturer\", \n",
    "                                          jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/Samsung\").get.body.text\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9_0\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Technology Manufacturer\"\u001b[0m, \u001b[32m0.8045666127888648\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"US States\"\u001b[0m, \u001b[32m0.12154673489951259\u001b[0m)\n",
       ")\n",
       "\u001b[36mres9_1\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Technology Manufacturer\"\u001b[0m, \u001b[32m0.28254515021851767\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"US States\"\u001b[0m, \u001b[32m0.03239471822374247\u001b[0m)\n",
       ")\n",
       "\u001b[36mres9_2\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Technology Manufacturer\"\u001b[0m, \u001b[32m0.5286973699815706\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"US States\"\u001b[0m, \u001b[32m0.0870747335436455\u001b[0m)\n",
       ")\n",
       "\u001b[36mres9_3\u001b[0m: \u001b[32mArray\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mDouble\u001b[0m)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"Technology Manufacturer\"\u001b[0m, \u001b[32m0.318008201282106\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m\"US States\"\u001b[0m, \u001b[32m-0.01576924662740763\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DocumentClassifier.classifyDocument(jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/Dell\").get.body.text)\n",
    "DocumentClassifier.classifyDocument(jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/Linear_Algebra\").get.body.text)\n",
    "DocumentClassifier.classifyDocument(jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/Tesla_Motors\").get.body.text)\n",
    "DocumentClassifier.classifyDocument(jsoup.Jsoup.connect(\"https://en.wikipedia.org/wiki/Farming\").get.body.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SparkEngine.sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "* This appears to be a reasonable way to implement a low-latency streaming document classifyer. \n",
    "* As observed with previous projects, cosine simmilarity appears to be an acceptable measure to compare records\n",
    "* The current implementation easily plugs into other applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
